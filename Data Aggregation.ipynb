{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLRH_Preprocessing_Complete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Enrico-Call/RL-AKI/blob/main/Data%20Aggregation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/AmsterdamUMC/AmsterdamUMCdb/blob/master/img/logo_amds.png?raw=1\" alt=\"Logo\" width=128px/>\n",
        "\n",
        "# VUmc Research Project - Reinforcement Learning for Sepsis Prevention\n",
        "# Data Aggregation\n",
        "\n",
        "AmsterdamUMCdb version 1.0.2 March 2020  \n",
        "Copyright &copy; 2003-2022 Amsterdam UMC - Amsterdam Medical Data Science"
      ],
      "metadata": {
        "id": "V02CVVzdNfej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set up the environment variables for Colab and GoogleBigQuery to access"
      ],
      "metadata": {
        "id": "R96zGqRiNpek"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0L2i-Nnp7_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22cbd02-87b4-4de7-a63f-7080f3d0328c"
      },
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "from IPython.display import display\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('use_inf_as_na', True)\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir('/content/drive/MyDrive/MLRFH')\n",
        " \n",
        "#sets dateset\n",
        "PROJECT_ID = 'rl-aki'\n",
        "DATASET_ID = 'version1_0_2'\n",
        "LOCATION = 'eu'\n",
        " \n",
        "#all libraries check this environment variable, so set it:\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        " \n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Preprocessing Functions"
      ],
      "metadata": {
        "id": "oHbqYVb-N6v_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyva7EbsU_yt"
      },
      "source": [
        "#Some preprocessing functions \n",
        "\n",
        "def to_cols(data):\n",
        "\n",
        "  grouped = data.pivot_table(index=['admissionid', 'time'], \n",
        "          columns=['item'], values='value')\n",
        "\n",
        "  return grouped\n",
        "  \n",
        "\n",
        "def to_cols_action(data):\n",
        "\n",
        "  grouped = data.pivot_table(index=['admissionid', 'time'], \n",
        "            columns=['item'], values='administered')\n",
        "\n",
        "  return grouped\n",
        "\n",
        "def remove_outliers(data):\n",
        "  #delete outliers\n",
        "  data = data.reset_index() #return to single index\n",
        "\n",
        "  #select outlier cols\n",
        "  all_cols = ['Kreatinine', 'Kreatinine (bloed)', 'KREAT enzym. (bloed)',\n",
        "       'UrineSupraPubis', 'UrineIncontinentie', 'Nefrodrain re Uit',\n",
        "       'Nefrodrain li Uit', 'UrineSpontaan', 'UrineUP', 'UrineSplint Re',\n",
        "       'UrineSplint Li', 'UrineCAD', 'Chloor (bloed)', 'Natrium (bloed)',\n",
        "       'Kalium (bloed)', 'HCO3', 'Natrium', 'Natrium Astrup',\n",
        "       'Kalium Astrup', 'Chloor Astrup', 'Chloor', 'Kalium',\n",
        "       'Act.HCO3 (bloed)', 'Na (onv.ISE) (bloed)', 'K (onv.ISE) (bloed)',\n",
        "       'Cl (onv.ISE) (bloed)', 'Niet invasieve bloeddruk gemiddeld',\n",
        "       'ABP gemiddeld II', 'ABP gemiddeld']\n",
        "  \n",
        "  # Natrium\n",
        "  data['Natrium'][(data['Natrium'] < 65.) & (data['Natrium'] > 165.)] = np.nan\n",
        "  data['Natrium (bloed)'][(data['Natrium (bloed)'] < 65.) & (data['Natrium (bloed)'] > 165.)] = np.nan\n",
        "  data['Natrium Astrup'][(data['Natrium Astrup'] < 65.) & (data['Natrium Astrup'] > 165.)] = np.nan\n",
        "  data['Na (onv.ISE) (bloed)'][(data['Na (onv.ISE) (bloed)'] < 65.) & (data['Na (onv.ISE) (bloed)'] > 165.)] = np.nan\n",
        "  \n",
        "  # Mean Blood Pressure\n",
        "  data['ABP gemiddeld'][(data['ABP gemiddeld'] < 30.) & (data['ABP gemiddeld'] > 165.)] = np.nan\n",
        "  data['Niet invasieve bloeddruk gemiddeld'][(data['Niet invasieve bloeddruk gemiddeld'] < 30.) & (data['Niet invasieve bloeddruk gemiddeld'] > 165.)] = np.nan\n",
        "  data['ABP gemiddeld II'][(data['ABP gemiddeld II'] < 30) & (data['ABP gemiddeld II'] > 165)]\n",
        "\n",
        "  # Kalium\n",
        "  data['Kalium'][data['Kalium'] > 12.] = np.nan\n",
        "  data['Kalium (bloed)'][data['Kalium (bloed)'] > 12.] = np.nan\n",
        "  data['Kalium Astrup'][data['Kalium Astrup'] > 12.] = np.nan\n",
        "  data['K (onv.ISE) (bloed)'][data['K (onv.ISE) (bloed)'] > 12.] = np.nan\n",
        "\n",
        "  # Kreatinine\n",
        "  data['Kreatinine'][data['Kreatinine'] < 30.] = np.nan\n",
        "  data['Kreatinine (bloed)'][data['Kreatinine (bloed)'] < 30.] = np.nan\n",
        "  data['KREAT enzym. (bloed)'][data['KREAT enzym. (bloed)'] < 30.] = np.nan\n",
        "\n",
        "  #make nans of all negative vals\n",
        "  data[all_cols] = data[all_cols].applymap(lambda x: np.nan if x < 0 else x)\n",
        "  return data\n",
        "\n",
        "\n",
        "def remove_outliers_action(data):\n",
        "\n",
        "  #delete outliers\n",
        "  outliers = data.reset_index() #return to single index\n",
        "\n",
        "  cols = ['Noradrenaline (Norepinefrine)', 'NaCl 0,45%/Glucose 2,5%']\n",
        "\n",
        "  #select outlier cols\n",
        "  data['Noradrenaline (Norepinefrine)'][data['Noradrenaline (Norepinefrine)'] > 10.] = np.nan\n",
        "  data['NaCl 0,45%/Glucose 2,5%'][data['NaCl 0,45%/Glucose 2,5%'] > 500.] = np.nan\n",
        "  \n",
        "\n",
        "  data = data[cols].applymap(lambda x: np.nan if x < 0 else x)\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "def get_4h(data):\n",
        "  #per patient, average the values in 4h timeslots\n",
        "\n",
        "  data = data.sort_values('time')\n",
        "  res = data.groupby([pd.Grouper('admissionid'),\n",
        "                        pd.Grouper(key ='time', freq='4H')\n",
        "                              \n",
        "  ]).mean()\n",
        "\n",
        "  return res\n",
        "\n",
        "def get_4h_urine(data):\n",
        "  #per patient, average the values in 4h timeslots\n",
        "\n",
        "  data = data.sort_values('time')\n",
        "  res = data.groupby([pd.Grouper('admissionid'),\n",
        "                        pd.Grouper(key ='time', freq='4H')\n",
        "                              \n",
        "  ]).sum()\n",
        "\n",
        "  return res\n",
        "\n",
        "def aggregate_col(data, colname):\n",
        "\n",
        "  #create new columns with cumulative count for consecutive nans\n",
        "  data['nancount'] = np.zeros(len(data))\n",
        "  data['nancount'] = data[colname].groupby((data[colname].notnull()).cumsum()).cumcount()\n",
        "                           \n",
        "  #manually set first row to 1 if nan since this is excluded in cumsum/count from line before\n",
        "  for i, v in enumerate(data[colname]):\n",
        "    if i == 0:\n",
        "      if np.isnan(v):\n",
        "        data[\"nancount\"] += 1\n",
        "  \n",
        "  #and all other ones +1, except non-nan values\n",
        "  data[\"nancount\"][data[colname] == np.nan] += 1\n",
        "\n",
        "  #set non-null values as 0 for nancount\n",
        "  data[\"nancount\"] = np.where(~data[colname].isnull(), 0, data[\"nancount\"])\n",
        "\n",
        "  #if value is not nan, then use previous value + 1 to get total cumulative nan count including the non-nan value\n",
        "  #this is the value we want to divide through to get the right value per hour\n",
        "\n",
        "  group_val = pd.DataFrame(data[colname]).reset_index()[colname]\n",
        "  group_count = pd.DataFrame(data[\"nancount\"]).reset_index()[\"nancount\"]\n",
        "\n",
        "  for i, v in enumerate(group_count):\n",
        "\n",
        "    if v == 0: #where no null values\n",
        " \n",
        "      if (i != 0): #first row: do nothing\n",
        "        if group_val[i] != 0: #value 0: do not divide\n",
        "          if group_count[i-1] != 0: #value before is not missing: do nothing\n",
        "          \n",
        "            group_val[i] = group_val[i] / (group_count[i-1]) #otherwise: divide through nancount of row before\n",
        "  \n",
        "\n",
        "  #and then fill backwards and return\n",
        "  return group_val.bfill()\n",
        "\n",
        "def sum_urine(data):\n",
        "  # urine_cols = ['UrineCAD']\n",
        "  urine_cols = ['UrineCAD', 'UrineSupraPubis', 'UrineUP', 'UrineSpontaan', 'UrineIncontinentie', 'UrineSplint Re', 'UrineSplint Li']\n",
        "  data['Urine_summed'] = data[urine_cols].sum(axis=1)\n",
        "  data['Urine_summed'] = np.where(data['Urine_summed'] == 0, np.nan, data['Urine_summed'])\n",
        "  return data.drop(columns=urine_cols)\n",
        "\n",
        "def aggregate_all_cols(data, space):\n",
        "\n",
        "  if space == 'state':\n",
        "\n",
        "    cols_to_agg = ['time', 'admissionid', 'Kreatinine', 'Kreatinine (bloed)', 'KREAT enzym. (bloed)',\n",
        "       'Nefrodrain re Uit', 'Nefrodrain li Uit', 'Chloor (bloed)', 'Natrium (bloed)',\n",
        "       'Kalium (bloed)', 'HCO3', 'Natrium', 'Natrium Astrup',\n",
        "       'Kalium Astrup', 'Chloor Astrup', 'Chloor', 'Kalium',\n",
        "       'Act.HCO3 (bloed)', 'Na (onv.ISE) (bloed)', 'K (onv.ISE) (bloed)',\n",
        "       'Cl (onv.ISE) (bloed)', 'Niet invasieve bloeddruk gemiddeld',\n",
        "       'ABP gemiddeld II', 'ABP gemiddeld']\n",
        "\n",
        "    #group urine (sum)\n",
        "    grouped = data.groupby('admissionid', as_index = False).apply(lambda x: aggregate_col(x, 'Urine_summed')).reset_index()['Urine_summed']\n",
        "    data['Urine'] = list(grouped.head(len(grouped)))\n",
        "    data = pd.DataFrame(data).reset_index()\n",
        "    urine_aggr = get_4h_urine(data[['admissionid', 'time', 'Urine']])\n",
        "\n",
        "    #group other variables (mean)\n",
        "    data[cols_to_agg] = data[cols_to_agg].bfill()\n",
        "    df_aggr = get_4h(data[cols_to_agg])\n",
        "\n",
        "    #combine both aggregations\n",
        "    combined = pd.concat([urine_aggr, df_aggr], axis=1)\n",
        "\n",
        "    return combined\n",
        "\n",
        "  if space == 'action':\n",
        "\n",
        "    data = data.reset_index()\n",
        "    cols_to_agg = ['time', 'admissionid', 'Dobutamine (Dobutrex)',\n",
        "                   'Adrenaline (Epinefrine)', 'Dopamine (Inotropin)',\n",
        "                   'Noradrenaline (Norepinefrine)', 'NaCl 0,45%/Glucose 2,5%']\n",
        "    data[cols_to_agg] = data[cols_to_agg].bfill()\n",
        "    df_aggr = get_4h(data[cols_to_agg])\n",
        "\n",
        "    return df_aggr\n",
        "\n",
        "  else:\n",
        "\n",
        "    print(\"ERROR INVALID SPACE TYPE: options for space: state, action\")\n",
        "\n",
        "\n",
        "def interpolate(data_agg):\n",
        "  #interpolate null values\n",
        "  return data_agg.interpolate(limit_direction='forward')\n",
        "\n",
        "\n",
        "def process_statespace(data):\n",
        "  data['time'] = pd.to_datetime(data['time'], unit='m', origin = 'unix')\n",
        "  grouped = to_cols(data)\n",
        "  grouped = remove_outliers(grouped)\n",
        "  data_sum = sum_urine(grouped)\n",
        "  data_agg = aggregate_all_cols(data_sum, space=\"state\")\n",
        "  #data_filled = interpolate(data_agg)\n",
        "  return data_agg.reset_index()\n",
        "\n",
        "  \n",
        "def process_actionspace(data):\n",
        "  # data['time'] = pd.to_datetime(data['stop'] - data['start'], unit='ms')\n",
        "  # data = data.drop(columns = ['start', 'stop'])\n",
        "  # data['time'] = pd.to_datetime(data['time'], unit='ms', origin = 'unix')\n",
        "  # grouped = to_cols_action(data)\n",
        "  # #grouped = remove_outliers_action(grouped)\n",
        "  # data_agg = aggregate_all_cols(grouped, space=\"action\")\n",
        "  # #data_filled = interpolate(data_agg)\n",
        "\n",
        "  # Extract Fluids and Vasopressors\n",
        "  fluids = data.loc[~data['itemid'].isin([7179,7178,6818,7229])]\n",
        "  vasop = data.loc[data['itemid'].isin([7179,7178,6818,7229])]\n",
        "  \n",
        "  # Perform Aggregation\n",
        "  df_aggr_fluids = transform_df(data=transform_daterange(fluids[['admissionid',\n",
        "                                                                 'fluidin',\n",
        "                                                                 'start_time',\n",
        "                                                                 'stop_time']].sort_values(['admissionid', 'start_time']).copy(),\n",
        "                                                     time_col = 'stop_time',\n",
        "                                                     infer_start_time=False,\n",
        "                                                     multi_source=False,\n",
        "                                                     start_time = 'start_time',\n",
        "                                                     end_time = 'stop_time',\n",
        "                                                     value_col = 'fluidin',\n",
        "                                                     group_col = ['admissionid']),\n",
        "                                 time_col='stop_time',\n",
        "                                 bins=range(0, 76*60, 4*60),\n",
        "                                 bin_labels=range(0, 72*60, 4*60),\n",
        "                                 group_cols=['admissionid', 'binn'],\n",
        "                                 agg_func={'fluid_sum': ('prod_fill', 'sum')})\n",
        "  df_aggr_vasops = transform_df(data=transform_daterange(vasop[['admissionid',\n",
        "                                                                 'fluidin',\n",
        "                                                                 'start_time',\n",
        "                                                                 'stop_time']].sort_values(['admissionid', 'start_time']).copy(),\n",
        "                                                     time_col = 'stop_time',\n",
        "                                                     infer_start_time=False,\n",
        "                                                     multi_source=False,\n",
        "                                                     start_time = 'start_time',\n",
        "                                                     end_time = 'stop_time',\n",
        "                                                     value_col = 'fluidin',\n",
        "                                                     group_col = ['admissionid']),\n",
        "                                 time_col='stop_time',\n",
        "                                 bins=range(0, 76*60, 4*60),\n",
        "                                 bin_labels=range(0, 72*60, 4*60),\n",
        "                                 group_cols=['admissionid', 'binn'],\n",
        "                                 agg_func={'vasops_sum': ('prod_fill', 'sum')})\n",
        "    \n",
        "  df_aggr_fluids['fluid_sum'] = df_aggr_fluids['fluid_sum'].fillna(0)\n",
        "  df_aggr_vasops['vasops_sum'] = df_aggr_vasops['vasops_sum'].fillna(0)\n",
        "\n",
        "  return pd.merge(df_aggr_fluids, df_aggr_vasops, how='outer', on=['admissionid', 'binn'])\n",
        "\n",
        "def transform_df(data: pd.DataFrame = None,\n",
        "                 time_col: str = 'time',\n",
        "                 bins: list = None,\n",
        "                 bin_labels: list = None,\n",
        "                 group_cols: list = ['admissionid', 'binn'],\n",
        "                 agg_func: dict = None):\n",
        "    \"\"\"\n",
        "    Transforms the input data from the AmsterdamUMCdb and return a dataframe with bins assigned to each record based on the time column\n",
        "    :param data: dataframe with single timestamps as integers, patientid and values\n",
        "    :param bins: list of bins to divide the timestamps in\n",
        "    :param bin_labels: list of labels to name the bins with\n",
        "    :param group_cols: list of column to group by, including the newly created 'binn'\n",
        "    :param agg_func: dictionary of kwargs passed to the .agg() method\n",
        "    \"\"\"\n",
        "    \n",
        "    data['binn'] = pd.cut(data[time_col], bins=bins, labels=bin_labels)\n",
        "    data = data[data[time_col]>=0]\n",
        "    grouped_data = data.groupby(group_cols).agg(**agg_func).reset_index().sort_values(by=group_cols, ascending=True)\n",
        "    \n",
        "    return grouped_data\n",
        "\n",
        "\n",
        "def transform_daterange(data: pd.DataFrame,\n",
        "                        time_col: str = 'time',\n",
        "                        infer_start_time: bool = True,\n",
        "                        multi_source: bool = False,\n",
        "                        multi_source_col: str = None,\n",
        "                        start_time: str = 'start_time',\n",
        "                        end_time: str = 'end_time',\n",
        "                        time_unit: str = 'm',\n",
        "                        value_col: str = 'value',\n",
        "                        group_col: list = None,\n",
        "                        fill_method: str = 'backfill',\n",
        "                        fill_lim: int = 540\n",
        "                        ):\n",
        "    \"\"\"\n",
        "    Transform interval data with single timestamps to time range, calculate production, resample and backward fill\n",
        "    :param data: dataframe with id, value and timestamp\n",
        "    :param time_col: string representing the column name for the time of registration in a single timestamp dataframe\n",
        "    :param infer_start_time: boolean representing whether the start time should be inferred from the previous record\n",
        "    :param start_time: string representing the column name with the record start time\n",
        "    :param end_time: string representing the column name with the record end time\n",
        "    :param time_unit: interpret the integer timestamp as the given time unit and convert back to this unit at the end\n",
        "    :param value_col: string representing the column name with the values of the measurements\n",
        "    :param group_col: list representing the ids of patients and/or products\n",
        "    :param fill_method: string to represent the method as used in pandas.series.fillna\n",
        "    :param fill_lim: integer to represent the number of time units to be filled\n",
        "    \"\"\"\n",
        "    \n",
        "    if group_col is None:\n",
        "        group_col = ['admissionid'] # PM: defining a list as default will keep alterations when rerunning the function\n",
        "    \n",
        "    # convert to datetime and set index to time column\n",
        "    data[time_col] = pd.to_datetime(data[time_col], unit=time_unit)\n",
        "    data[start_time] = pd.to_datetime(data[start_time], unit=time_unit)\n",
        "    \n",
        "    if infer_start_time:\n",
        "        # get start time from previous record\n",
        "        data['start_time'] = data.groupby(group_col)[time_col].shift(1)\n",
        "        start_time = 'start_time'\n",
        "        end_time = time_col\n",
        "    else:\n",
        "        # transform other columns to datetime if they exist and are still integer type, otherwise leave as is\n",
        "        for t_col in [start_time, end_time]:\n",
        "            if t_col in data:\n",
        "                if pd.api.types.is_integer_dtype(data[start_time]):\n",
        "                    data[t_col] = pd.to_datetime(data[t_col], unit=time_unit)\n",
        "    \n",
        "    # get time difference from start and end times   \n",
        "    data['time_diff'] = (data[end_time] - data[start_time]) / np.timedelta64(1, time_unit)\n",
        "\n",
        "    if multi_source:\n",
        "        if multi_source_col is None:\n",
        "            # give each record a unique id to group by in order to handle simultaneous records\n",
        "            data['administrationid'] = range(data.shape[0])\n",
        "            group_col += ['administrationid']\n",
        "        else:\n",
        "            group_col += [multi_source_col]\n",
        "    \n",
        "    # get production per time unit\n",
        "    data['prod'] = data[value_col] / data['time_diff']\n",
        "    \n",
        "    # if start and end time are registered in the same record, create a new record with the other value as index\n",
        "    if infer_start_time:\n",
        "        data_merged = data.copy()\n",
        "        data_merged.index = data_merged[time_col]\n",
        "    else:\n",
        "        data_end = data.copy()\n",
        "        data.index = data.start_time\n",
        "        data_end.index = data_end.stop_time\n",
        "        data_merged = pd.concat([data, data_end]).sort_values(group_col + [start_time, end_time])\n",
        "    \n",
        "    # resample for each unit\n",
        "    res = data_merged[group_col + ['prod', start_time, end_time]].groupby(group_col).resample('1T').mean().drop(group_col, axis=1, errors='ignore').reset_index().copy()\n",
        "    \n",
        "    # fill missing values\n",
        "    res['prod_fill'] = res.groupby(group_col)['prod'].fillna(method=fill_method, limit=fill_lim) #9 hours\n",
        "    \n",
        "    # reset time column to integer values\n",
        "    transform_time_col = {'s': 1, 'm': 60, 'h': 3600, 'd': 86_400}\n",
        "    if infer_start_time:\n",
        "        res[time_col] = (res[time_col].view(np.int64) / (transform_time_col.get(time_unit) * 1_000_000_000)).astype(int)\n",
        "    else:\n",
        "        if multi_source:\n",
        "            level_col = 'level_2'\n",
        "        else:\n",
        "            level_col = 'level_1'\n",
        "        res[time_col] = (res[level_col].view(np.int64) / (transform_time_col.get(time_unit) * 1_000_000_000)).astype(int)\n",
        "    \n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Import Dataframes and Perform Aggregation"
      ],
      "metadata": {
        "id": "JZzls00gOCbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "statespace = pd.read_csv('final_state_space.csv')\n",
        "actionspace = pd.read_csv('final_action_space.csv')\n",
        "\n",
        "# state = process_statespace(statespace)\n",
        "# action = process_actionspace(actionspace)"
      ],
      "metadata": {
        "id": "RmV76_AOCy47"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = process_statespace(statespace)"
      ],
      "metadata": {
        "id": "UCrXRf2QUELV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action = process_actionspace(actionspace)"
      ],
      "metadata": {
        "id": "P8QlSfr9Ru2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Save Aggregated Dataframe on Drive"
      ],
      "metadata": {
        "id": "muHoNB95Ofum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated.to_csv('aggregated.csv')"
      ],
      "metadata": {
        "id": "8NYG5CrvOmq1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}