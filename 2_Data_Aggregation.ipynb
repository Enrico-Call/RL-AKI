{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLRH_Preprocessing_Complete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Enrico-Call/RL-AKI/blob/main/2_Data_Aggregation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/AmsterdamUMC/AmsterdamUMCdb/blob/master/img/logo_amds.png?raw=1\" alt=\"Logo\" width=128px/>\n",
        "\n",
        "# VUmc Research Project - Reinforcement Learning for Sepsis Prevention\n",
        "# Data Aggregation\n",
        "\n",
        "AmsterdamUMCdb version 1.0.2 March 2020  \n",
        "Copyright &copy; 2003-2022 Amsterdam UMC - Amsterdam Medical Data Science"
      ],
      "metadata": {
        "id": "V02CVVzdNfej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set up the environment variables for Colab and GoogleBigQuery to access"
      ],
      "metadata": {
        "id": "R96zGqRiNpek"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0L2i-Nnp7_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3512b661-f164-4f4c-8dde-4ff75386cf05"
      },
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "from IPython.display import display\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('use_inf_as_na', True)\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir('/content/drive/MyDrive/MLRFH')\n",
        " \n",
        "#sets dateset\n",
        "PROJECT_ID = 'rl-aki'\n",
        "DATASET_ID = 'version1_0_2'\n",
        "LOCATION = 'eu'\n",
        " \n",
        "#all libraries check this environment variable, so set it:\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        " \n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Preprocessing Functions"
      ],
      "metadata": {
        "id": "oHbqYVb-N6v_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyva7EbsU_yt"
      },
      "source": [
        "#Some preprocessing functions \n",
        "\n",
        "def to_cols(data):\n",
        "\n",
        "  grouped = data.pivot_table(index=['admissionid', 'time'], \n",
        "          columns=['item'], values='value')\n",
        "\n",
        "  return grouped\n",
        "  \n",
        "\n",
        "def to_cols_action(data):\n",
        "\n",
        "  grouped = data.pivot_table(index=['admissionid', 'time'], \n",
        "            columns=['item'], values='administered')\n",
        "\n",
        "  return grouped\n",
        "\n",
        "def remove_outliers(data):\n",
        "  #delete outliers\n",
        "  data = data.reset_index() #return to single index\n",
        "\n",
        "  #select outlier cols\n",
        "  all_cols = ['Kreatinine', 'Kreatinine (bloed)', 'KREAT enzym. (bloed)',\n",
        "       'UrineSupraPubis', 'UrineIncontinentie', 'Nefrodrain re Uit',\n",
        "       'Nefrodrain li Uit', 'UrineSpontaan', 'UrineUP', 'UrineSplint Re',\n",
        "       'UrineSplint Li', 'UrineCAD', 'Chloor (bloed)', 'Natrium (bloed)',\n",
        "       'Kalium (bloed)', 'HCO3', 'Natrium', 'Natrium Astrup',\n",
        "       'Kalium Astrup', 'Chloor Astrup', 'Chloor', 'Kalium',\n",
        "       'Act.HCO3 (bloed)', 'Na (onv.ISE) (bloed)', 'K (onv.ISE) (bloed)',\n",
        "       'Cl (onv.ISE) (bloed)', 'Niet invasieve bloeddruk gemiddeld',\n",
        "       'ABP gemiddeld II', 'ABP gemiddeld']\n",
        "  \n",
        "  # Natrium\n",
        "  data['Natrium'][(data['Natrium'] < 65.) & (data['Natrium'] > 165.)] = np.nan\n",
        "  data['Natrium (bloed)'][(data['Natrium (bloed)'] < 65.) & (data['Natrium (bloed)'] > 165.)] = np.nan\n",
        "  data['Natrium Astrup'][(data['Natrium Astrup'] < 65.) & (data['Natrium Astrup'] > 165.)] = np.nan\n",
        "  data['Na (onv.ISE) (bloed)'][(data['Na (onv.ISE) (bloed)'] < 65.) & (data['Na (onv.ISE) (bloed)'] > 165.)] = np.nan\n",
        "  \n",
        "  # Mean Blood Pressure\n",
        "  data['ABP gemiddeld'][(data['ABP gemiddeld'] < 30.) & (data['ABP gemiddeld'] > 165.)] = np.nan\n",
        "  data['Niet invasieve bloeddruk gemiddeld'][(data['Niet invasieve bloeddruk gemiddeld'] < 30.) & (data['Niet invasieve bloeddruk gemiddeld'] > 165.)] = np.nan\n",
        "  data['ABP gemiddeld II'][(data['ABP gemiddeld II'] < 30) & (data['ABP gemiddeld II'] > 165)]\n",
        "\n",
        "  # Kalium\n",
        "  data['Kalium'][data['Kalium'] > 12.] = np.nan\n",
        "  data['Kalium (bloed)'][data['Kalium (bloed)'] > 12.] = np.nan\n",
        "  data['Kalium Astrup'][data['Kalium Astrup'] > 12.] = np.nan\n",
        "  data['K (onv.ISE) (bloed)'][data['K (onv.ISE) (bloed)'] > 12.] = np.nan\n",
        "\n",
        "  # Kreatinine\n",
        "  data['Kreatinine'][data['Kreatinine'] < 30.] = np.nan\n",
        "  data['Kreatinine (bloed)'][data['Kreatinine (bloed)'] < 30.] = np.nan\n",
        "  data['KREAT enzym. (bloed)'][data['KREAT enzym. (bloed)'] < 30.] = np.nan\n",
        "\n",
        "  # Bicarbonate\n",
        "  data['HCO3'][(data['HCO3'] < 5.) & (data['HCO3'] > 45.)] = np.nan\n",
        "  data['Act.HCO3 (bloed)'][(data['Act.HCO3 (bloed)'] < 5.) & (data['Act.HCO3 (bloed)'] > 45.)] = np.nan\n",
        "\n",
        "  # Chloor\n",
        "  data['Chloor'][(data['Chloor'] < 50.) & (data['Chloor'] > 200.)] = np.nan\n",
        "  data['Chloor (bloed)'][(data['Chloor (bloed)'] < 50.) & (data['Chloor (bloed)'] > 200.)] = np.nan\n",
        "  data['Chloor Astrup'][(data['Chloor Astrup'] < 50.) & (data['Chloor Astrup'] > 200.)] = np.nan\n",
        "  data['Cl (onv.ISE) (bloed)'][(data['Cl (onv.ISE) (bloed)'] < 50.) & (data['Cl (onv.ISE) (bloed)'] > 200.)] = np.nan\n",
        "\n",
        "  #make nans of all negative vals\n",
        "  data[all_cols] = data[all_cols].applymap(lambda x: np.nan if x < 0 else x)\n",
        "  return data\n",
        "\n",
        "\n",
        "def remove_outliers_action(data):\n",
        "\n",
        "  #delete outliers\n",
        "  outliers = data.reset_index() #return to single index\n",
        "\n",
        "  cols = ['Noradrenaline (Norepinefrine)', 'NaCl 0,45%/Glucose 2,5%']\n",
        "\n",
        "  #select outlier cols\n",
        "  data['Noradrenaline (Norepinefrine)'][data['Noradrenaline (Norepinefrine)'] > 10.] = np.nan\n",
        "  data['NaCl 0,45%/Glucose 2,5%'][data['NaCl 0,45%/Glucose 2,5%'] > 500.] = np.nan\n",
        "  \n",
        "\n",
        "  data = data[cols].applymap(lambda x: np.nan if x < 0 else x)\n",
        "\n",
        "  return data\n",
        "\n",
        "def get_4h(data):\n",
        "  #per patient, average the values in 4h timeslots\n",
        "\n",
        "  data = data.sort_values('time')\n",
        "  res = data.groupby([pd.Grouper('admissionid'),\n",
        "                        pd.Grouper(key ='time', freq='4H')\n",
        "                              \n",
        "  ]).mean()\n",
        "\n",
        "  return res\n",
        "\n",
        "def get_4h_urine(data):\n",
        "  #per patient, average the values in 4h timeslots\n",
        "\n",
        "  data = data.sort_values('time')\n",
        "  res = data.groupby([pd.Grouper('admissionid'),\n",
        "                        pd.Grouper(key ='time', freq='4H')\n",
        "                              \n",
        "  ]).sum()\n",
        "\n",
        "  return res\n",
        "\n",
        "def aggregate_col(data, colname):\n",
        "\n",
        "  #create new columns with cumulative count for consecutive nans\n",
        "  data['nancount'] = np.zeros(len(data))\n",
        "  data['nancount'] = data[colname].groupby((data[colname].notnull()).cumsum()).cumcount()\n",
        "                           \n",
        "  #manually set first row to 1 if nan since this is excluded in cumsum/count from line before\n",
        "  for i, v in enumerate(data[colname]):\n",
        "    if i == 0:\n",
        "      if np.isnan(v):\n",
        "        data[\"nancount\"] += 1\n",
        "  \n",
        "  #and all other ones +1, except non-nan values\n",
        "  data[\"nancount\"][data[colname] == np.nan] += 1\n",
        "\n",
        "  #set non-null values as 0 for nancount\n",
        "  data[\"nancount\"] = np.where(~data[colname].isnull(), 0, data[\"nancount\"])\n",
        "\n",
        "  #if value is not nan, then use previous value + 1 to get total cumulative nan count including the non-nan value\n",
        "  #this is the value we want to divide through to get the right value per hour\n",
        "\n",
        "  group_val = pd.DataFrame(data[colname]).reset_index()[colname]\n",
        "  group_count = pd.DataFrame(data[\"nancount\"]).reset_index()[\"nancount\"]\n",
        "\n",
        "  for i, v in enumerate(group_count):\n",
        "\n",
        "    if v == 0: #where no null values\n",
        " \n",
        "      if (i != 0): #first row: do nothing\n",
        "        if group_val[i] != 0: #value 0: do not divide\n",
        "          if group_count[i-1] != 0: #value before is not missing: do nothing\n",
        "          \n",
        "            group_val[i] = group_val[i] / (group_count[i-1]) #otherwise: divide through nancount of row before\n",
        "  \n",
        "\n",
        "  #and then fill backwards and return\n",
        "  return group_val.bfill()\n",
        "\n",
        "def sum_urine(data):\n",
        "  # urine_cols = ['UrineCAD']\n",
        "  urine_cols = ['UrineCAD', 'UrineSupraPubis', 'UrineUP', 'UrineSpontaan', 'UrineIncontinentie', 'UrineSplint Re', 'UrineSplint Li']\n",
        "  data['Urine_summed'] = data[urine_cols].sum(axis=1)\n",
        "  data['Urine_summed'] = np.where(data['Urine_summed'] == 0, np.nan, data['Urine_summed'])\n",
        "  return data.drop(columns=urine_cols)\n",
        "\n",
        "def aggregate_all_cols(data, space):\n",
        "\n",
        "  if space == 'state':\n",
        "\n",
        "    cols_to_agg = ['time', 'admissionid', 'Kreatinine', 'Kreatinine (bloed)', 'KREAT enzym. (bloed)',\n",
        "       'Chloor (bloed)', 'Natrium (bloed)',\n",
        "       'Kalium (bloed)', 'HCO3', 'Natrium', 'Natrium Astrup',\n",
        "       'Kalium Astrup', 'Chloor Astrup', 'Chloor', 'Kalium',\n",
        "       'Act.HCO3 (bloed)', 'Na (onv.ISE) (bloed)', 'K (onv.ISE) (bloed)',\n",
        "       'Cl (onv.ISE) (bloed)', 'Niet invasieve bloeddruk gemiddeld',\n",
        "       'ABP gemiddeld II', 'ABP gemiddeld']\n",
        "\n",
        "    #group urine (sum)\n",
        "    grouped = data.groupby('admissionid', as_index = False).apply(lambda x: aggregate_col(x, 'Urine_summed')).reset_index()['Urine_summed']\n",
        "    data['Urine'] = list(grouped.head(len(grouped)))\n",
        "    data = pd.DataFrame(data).reset_index()\n",
        "    urine_aggr = get_4h_urine(data[['admissionid', 'time', 'Urine']])\n",
        "\n",
        "    #group other variables (mean)\n",
        "    data[cols_to_agg] = data[cols_to_agg].bfill()\n",
        "    df_aggr = get_4h(data[cols_to_agg])\n",
        "\n",
        "    #combine both aggregations\n",
        "    combined = pd.concat([urine_aggr, df_aggr], axis=1)\n",
        "\n",
        "    return combined\n",
        "\n",
        "  if space == 'action':\n",
        "\n",
        "    data = data.reset_index()\n",
        "    cols_to_agg = ['time', 'admissionid', 'Dobutamine (Dobutrex)',\n",
        "                   'Adrenaline (Epinefrine)', 'Dopamine (Inotropin)',\n",
        "                   'Noradrenaline (Norepinefrine)', 'NaCl 0,45%/Glucose 2,5%']\n",
        "    data[cols_to_agg] = data[cols_to_agg].bfill()\n",
        "    df_aggr = get_4h(data[cols_to_agg])\n",
        "\n",
        "    return df_aggr\n",
        "\n",
        "  else:\n",
        "\n",
        "    print(\"ERROR INVALID SPACE TYPE: options for space: state, action\")\n",
        "\n",
        "\n",
        "def interpolate(data_agg):\n",
        "  #interpolate null values\n",
        "  return data_agg.interpolate(limit_direction='forward')\n",
        "\n",
        "def transform_df(data: pd.DataFrame = None,\n",
        "                 time_col: str = 'time',\n",
        "                 bins: list = None,\n",
        "                 bin_labels: list = None,\n",
        "                 group_cols: list = ['admissionid', 'binn'],\n",
        "                 agg_func: dict = None):\n",
        "    \"\"\"\n",
        "    Transforms the input data from the AmsterdamUMCdb and return a dataframe with bins assigned to each record based on the time column\n",
        "    :param data: dataframe with single timestamps as integers, patientid and values\n",
        "    :param bins: list of bins to divide the timestamps in\n",
        "    :param bin_labels: list of labels to name the bins with\n",
        "    :param group_cols: list of column to group by, including the newly created 'binn'\n",
        "    :param agg_func: dictionary of kwargs passed to the .agg() method\n",
        "    \"\"\"\n",
        "    \n",
        "    data['binn'] = pd.cut(data[time_col], bins=bins, labels=bin_labels)\n",
        "    data = data[data[time_col]>=0]\n",
        "    grouped_data = data.groupby(group_cols).agg(**agg_func).reset_index().sort_values(by=group_cols, ascending=True)\n",
        "    \n",
        "    return grouped_data\n",
        "\n",
        "\n",
        "def transform_daterange(data: pd.DataFrame,\n",
        "                        time_col: str = 'time',\n",
        "                        infer_start_time: bool = True,\n",
        "                        multi_source: bool = False,\n",
        "                        multi_source_col: str = None,\n",
        "                        start_time: str = 'start_time',\n",
        "                        end_time: str = 'end_time',\n",
        "                        time_unit: str = 'm',\n",
        "                        value_col: str = 'value',\n",
        "                        group_col: list = None,\n",
        "                        fill_method: str = 'backfill',\n",
        "                        fill_lim: int = 540\n",
        "                        ):\n",
        "    \"\"\"\n",
        "    Transform interval data with single timestamps to time range, calculate production, resample and backward fill\n",
        "    :param data: dataframe with id, value and timestamp\n",
        "    :param time_col: string representing the column name for the time of registration in a single timestamp dataframe\n",
        "    :param infer_start_time: boolean representing whether the start time should be inferred from the previous record\n",
        "    :param start_time: string representing the column name with the record start time\n",
        "    :param end_time: string representing the column name with the record end time\n",
        "    :param time_unit: interpret the integer timestamp as the given time unit and convert back to this unit at the end\n",
        "    :param value_col: string representing the column name with the values of the measurements\n",
        "    :param group_col: list representing the ids of patients and/or products\n",
        "    :param fill_method: string to represent the method as used in pandas.series.fillna\n",
        "    :param fill_lim: integer to represent the number of time units to be filled\n",
        "    \"\"\"\n",
        "    \n",
        "    if group_col is None:\n",
        "        group_col = ['admissionid'] # PM: defining a list as default will keep alterations when rerunning the function\n",
        "    \n",
        "    # convert to datetime and set index to time column\n",
        "    data[time_col] = pd.to_datetime(data[time_col], unit=time_unit)\n",
        "    data[start_time] = pd.to_datetime(data[start_time], unit=time_unit)\n",
        "    \n",
        "    if infer_start_time:\n",
        "        # get start time from previous record\n",
        "        data['start_time'] = data.groupby(group_col)[time_col].shift(1)\n",
        "        start_time = 'start_time'\n",
        "        end_time = time_col\n",
        "    else:\n",
        "        # transform other columns to datetime if they exist and are still integer type, otherwise leave as is\n",
        "        for t_col in [start_time, end_time]:\n",
        "            if t_col in data:\n",
        "                if pd.api.types.is_integer_dtype(data[start_time]):\n",
        "                    data[t_col] = pd.to_datetime(data[t_col], unit=time_unit)\n",
        "    \n",
        "    # get time difference from start and end times   \n",
        "    data['time_diff'] = (data[end_time] - data[start_time]) / np.timedelta64(1, time_unit)\n",
        "\n",
        "    if multi_source:\n",
        "        if multi_source_col is None:\n",
        "            # give each record a unique id to group by in order to handle simultaneous records\n",
        "            data['administrationid'] = range(data.shape[0])\n",
        "            group_col += ['administrationid']\n",
        "        else:\n",
        "            group_col += [multi_source_col]\n",
        "    \n",
        "    # get production per time unit\n",
        "    data['prod'] = data[value_col] / data['time_diff']\n",
        "    \n",
        "    # if start and end time are registered in the same record, create a new record with the other value as index\n",
        "    if infer_start_time:\n",
        "        data_merged = data.copy()\n",
        "        data_merged.index = data_merged[time_col]\n",
        "    else:\n",
        "        data_end = data.copy()\n",
        "        data.index = data.start_time\n",
        "        data_end.index = data_end.stop_time\n",
        "        data_merged = pd.concat([data, data_end]).sort_values(group_col + [start_time, end_time])\n",
        "    \n",
        "    # resample for each unit\n",
        "    res = data_merged[group_col + ['prod', start_time, end_time]].groupby(group_col).resample('1T').mean().drop(group_col, axis=1, errors='ignore').reset_index().copy()\n",
        "    \n",
        "    # fill missing values\n",
        "    res['prod_fill'] = res.groupby(group_col)['prod'].fillna(method=fill_method, limit=fill_lim) #9 hours\n",
        "    \n",
        "    # reset time column to integer values\n",
        "    transform_time_col = {'s': 1, 'm': 60, 'h': 3600, 'd': 86_400}\n",
        "    if infer_start_time:\n",
        "        res[time_col] = (res[time_col].view(np.int64) / (transform_time_col.get(time_unit) * 1_000_000_000)).astype(int)\n",
        "    else:\n",
        "        if multi_source:\n",
        "            level_col = 'level_2'\n",
        "        else:\n",
        "            level_col = 'level_1'\n",
        "        res[time_col] = (res[level_col].view(np.int64) / (transform_time_col.get(time_unit) * 1_000_000_000)).astype(int)\n",
        "    \n",
        "    return res\n",
        "    \n",
        "\n",
        "def get_demograhics(data, admissionid):\n",
        "  # Get gender\n",
        "  gender = data['gender'][data['admissionid'] == admissionid].head(1).item()\n",
        "  if gender == 'Man':\n",
        "    gender = 0\n",
        "  elif gender == 'Vrouw':\n",
        "    gender = 1\n",
        "  else:\n",
        "    gender = 'Unknown'\n",
        "\n",
        "  # Get Age\n",
        "  age = data['agegroup'][data['admissionid'] == admissionid].head(1).item()\n",
        "\n",
        "  # Get Weight\n",
        "  weight = data['weightgroup'][data['admissionid'] == admissionid].head(1).item()\n",
        "\n",
        "  # Get Height\n",
        "  height = data['heightgroup'][data['admissionid'] == admissionid].head(1).item()\n",
        "\n",
        "  # Get PatientID\n",
        "  patientID = data['patientid'][data['admissionid'] == admissionid].head(1).item()\n",
        "\n",
        "  # Get Date of Death\n",
        "  death = data['dateofdeath'][data['admissionid'] == admissionid].head(1).item()\n",
        "  if death == np.nan:\n",
        "    death = 0\n",
        "  else:\n",
        "    death = 1\n",
        "\n",
        "  return gender, age, weight, height, patientID, death\n",
        "\n",
        "\n",
        "def complete_state(data, df):\n",
        "  data = data.sort_values(by=['admissionid', 'time']).reset_index()\n",
        "  genders = []\n",
        "  ages = []\n",
        "  weights = []\n",
        "  heights = []\n",
        "  patientids = []\n",
        "  deaths = []\n",
        "\n",
        "  # Get demographics for each admission id\n",
        "  for x in data['admissionid']:\n",
        "    gender, age, weight, height, patientID, death = get_demograhics(df, x)\n",
        "    genders.append(gender)\n",
        "    ages.append(age)\n",
        "    weights.append(weight)\n",
        "    heights.append(height)\n",
        "    patientids.append(patientID)\n",
        "    deaths.append(death)\n",
        "\n",
        "  data['gender'] = genders\n",
        "  data['agegroup'] = ages\n",
        "  data['weightgroup'] = weights\n",
        "  data['heightgroup'] = heights\n",
        "  data['patientid'] = patientids\n",
        "  data['death'] = deaths\n",
        "\n",
        "  # Transform categorical variables into numerical\n",
        "  categories = {\"agegroup\": {\"18-39\": 1, \"40-49\": 2, \"50-59\": 3, \"60-69\":4, \"70-79\":5, \"80+\":6}, \n",
        "          \"weightgroup\": {'59-': 1,'60-69': 2, '80-89': 3, '70-79': 4, '90-99': 5, '100-109': 6, '110+': 7}, \n",
        "          \"heightroup\": {'159-': 1, '160-169': 2, '170-179': 3, '180-189': 4, '190+': 5}}\n",
        "  data = data.replace(categories)\n",
        "  \n",
        "  return data\n",
        "\n",
        "\n",
        "def process_statespace(data):\n",
        "  data['time'] = pd.to_datetime(data['time'], unit='m', origin = 'unix')\n",
        "  grouped = to_cols(data)\n",
        "  grouped = remove_outliers(grouped)\n",
        "  data_sum = sum_urine(grouped)\n",
        "  data_agg = aggregate_all_cols(data_sum, space=\"state\")\n",
        "  data_agg = complete_state(data_agg, data)\n",
        "  #data_filled = interpolate(data_agg)\n",
        "  return data_agg.reset_index()\n",
        "\n",
        "  \n",
        "def process_actionspace(data):\n",
        "  # data['time'] = pd.to_datetime(data['stop'] - data['start'], unit='ms')\n",
        "  # data = data.drop(columns = ['start', 'stop'])\n",
        "  # data['time'] = pd.to_datetime(data['time'], unit='ms', origin = 'unix')\n",
        "  # grouped = to_cols_action(data)\n",
        "  # #grouped = remove_outliers_action(grouped)\n",
        "  # data_agg = aggregate_all_cols(grouped, space=\"action\")\n",
        "  # #data_filled = interpolate(data_agg)\n",
        "\n",
        "  # Extract Fluids and Vasopressors\n",
        "  fluids = data.loc[~data['itemid'].isin([7179,7178,6818,7229])]\n",
        "  vasop = data.loc[data['itemid'].isin([7179,7178,6818,7229])]\n",
        "  \n",
        "  # Perform Aggregation\n",
        "  df_aggr_fluids = transform_df(data=transform_daterange(fluids[['admissionid',\n",
        "                                                                 'fluidin',\n",
        "                                                                 'start_time',\n",
        "                                                                 'stop_time']].sort_values(['admissionid', 'start_time']).copy(),\n",
        "                                                     time_col = 'stop_time',\n",
        "                                                     infer_start_time=False,\n",
        "                                                     multi_source=False,\n",
        "                                                     start_time = 'start_time',\n",
        "                                                     end_time = 'stop_time',\n",
        "                                                     value_col = 'fluidin',\n",
        "                                                     group_col = ['admissionid']),\n",
        "                                 time_col='stop_time',\n",
        "                                 bins=range(0, 76*60, 4*60),\n",
        "                                 bin_labels=range(0, 72*60, 4*60),\n",
        "                                 group_cols=['admissionid', 'binn'],\n",
        "                                 agg_func={'fluid_sum': ('prod_fill', 'sum')})\n",
        "  df_aggr_vasops = transform_df(data=transform_daterange(vasop[['admissionid',\n",
        "                                                                 'fluidin',\n",
        "                                                                 'start_time',\n",
        "                                                                 'stop_time']].sort_values(['admissionid', 'start_time']).copy(),\n",
        "                                                     time_col = 'stop_time',\n",
        "                                                     infer_start_time=False,\n",
        "                                                     multi_source=False,\n",
        "                                                     start_time = 'start_time',\n",
        "                                                     end_time = 'stop_time',\n",
        "                                                     value_col = 'fluidin',\n",
        "                                                     group_col = ['admissionid']),\n",
        "                                 time_col='stop_time',\n",
        "                                 bins=range(0, 76*60, 4*60),\n",
        "                                 bin_labels=range(0, 72*60, 4*60),\n",
        "                                 group_cols=['admissionid', 'binn'],\n",
        "                                 agg_func={'vasops_sum': ('prod_fill', 'sum')})\n",
        "    \n",
        "  df_aggr_fluids['fluid_sum'] = df_aggr_fluids['fluid_sum'].fillna(0)\n",
        "  df_aggr_vasops['vasops_sum'] = df_aggr_vasops['vasops_sum'].fillna(0)\n",
        "  df_aggr = pd.merge(df_aggr_fluids, df_aggr_vasops, how='outer', on=['admissionid', 'binn'])\n",
        "  df_aggr['time'] = pd.to_datetime(df_aggr['binn'].astype(float), unit='m', origin = 'unix')\n",
        "\n",
        "  return df_aggr"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Import Dataframes and Perform Aggregation"
      ],
      "metadata": {
        "id": "JZzls00gOCbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "statespace = pd.read_csv('final_state_space.csv')\n",
        "actionspace = pd.read_csv('final_action_space.csv')\n",
        "\n",
        "state = process_statespace(statespace)\n",
        "action = process_actionspace(actionspace)\n",
        "\n",
        "#merge datasets --> left merge on actionspace, as states without actions are not useful for our model\n",
        "aggregated = action.merge(state, on=[\"admissionid\", \"time\"], how=\"left\")\n",
        "\n",
        "#all the null values are patients that are not in the state space and only in the action space --> we cannot use them so they are dropped\n",
        "aggregated.isnull().sum()\n",
        "aggregated = aggregated.dropna()"
      ],
      "metadata": {
        "id": "RmV76_AOCy47"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated"
      ],
      "metadata": {
        "id": "HdWOXJvxLTT1",
        "outputId": "3a591122-511d-4dbe-d568-b4ae2db5da04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       admissionid  binn    fluid_sum  vasops_sum                time  \\\n",
              "0               11     0  1564.114410  127.987797 1970-01-01 00:00:00   \n",
              "1               11   240  1081.298935  189.248434 1970-01-01 04:00:00   \n",
              "2               11   480   564.661661  198.434211 1970-01-01 08:00:00   \n",
              "3               11   720  4650.132862  155.536134 1970-01-01 12:00:00   \n",
              "4               11   960  1158.447309  160.000000 1970-01-01 16:00:00   \n",
              "...            ...   ...          ...         ...                 ...   \n",
              "45737        21038  4080  6249.225680    0.000000 1970-01-03 20:00:00   \n",
              "45738        21049     0  8151.945003   13.393176 1970-01-01 00:00:00   \n",
              "45739        21049   240   590.869801   18.032787 1970-01-01 04:00:00   \n",
              "45740        21049   480  2821.231277   17.821038 1970-01-01 08:00:00   \n",
              "45741        21049   720  1255.125398   17.899379 1970-01-01 12:00:00   \n",
              "\n",
              "         index       Urine  Kreatinine  Kreatinine (bloed)  \\\n",
              "0          0.0   10.666667       439.0               342.2   \n",
              "1          1.0   10.666667       439.0               337.0   \n",
              "2          2.0   10.666667       439.0               316.0   \n",
              "3          3.0    9.633333       439.0               302.0   \n",
              "4          4.0   15.500000       439.0               302.0   \n",
              "...        ...         ...         ...                 ...   \n",
              "45737  39364.0  860.000000       281.0                81.0   \n",
              "45738  39365.0  200.000000       281.0                81.0   \n",
              "45739  39366.0  218.000000       281.0                81.0   \n",
              "45740  39367.0  830.000000       281.0                81.0   \n",
              "45741  39368.0  128.666667       281.0                81.0   \n",
              "\n",
              "       KREAT enzym. (bloed)  ...  Cl (onv.ISE) (bloed)  \\\n",
              "0                331.000000  ...                 114.0   \n",
              "1                331.000000  ...                 114.0   \n",
              "2                331.000000  ...                 114.0   \n",
              "3                331.000000  ...                 114.0   \n",
              "4                331.000000  ...                 114.0   \n",
              "...                     ...  ...                   ...   \n",
              "45737             60.000000  ...                 118.0   \n",
              "45738             59.285714  ...                 118.0   \n",
              "45739             62.000000  ...                 118.0   \n",
              "45740             71.714286  ...                 118.0   \n",
              "45741             81.000000  ...                 118.0   \n",
              "\n",
              "       Niet invasieve bloeddruk gemiddeld  ABP gemiddeld II  ABP gemiddeld  \\\n",
              "0                                    70.0              61.0      64.000000   \n",
              "1                                    70.0              61.0      72.400000   \n",
              "2                                    70.0              61.0      71.200000   \n",
              "3                                    70.0              61.0      66.750000   \n",
              "4                                    70.0              61.0      63.800000   \n",
              "...                                   ...               ...            ...   \n",
              "45737                                72.0              99.0      90.000000   \n",
              "45738                                72.0              99.0      68.857143   \n",
              "45739                                72.0              99.0      67.500000   \n",
              "45740                                72.0              99.0      69.714286   \n",
              "45741                                72.0              99.0      71.200000   \n",
              "\n",
              "       gender  agegroup  weightgroup  heightgroup  patientid  death  \n",
              "0           0       4.0          3.0      180-189       11.0    1.0  \n",
              "1           0       4.0          3.0      180-189       11.0    1.0  \n",
              "2           0       4.0          3.0      180-189       11.0    1.0  \n",
              "3           0       4.0          3.0      180-189       11.0    1.0  \n",
              "4           0       4.0          3.0      180-189       11.0    1.0  \n",
              "...       ...       ...          ...          ...        ...    ...  \n",
              "45737       1       1.0          4.0      160-169    18163.0    1.0  \n",
              "45738       0       5.0          2.0      160-169    18171.0    1.0  \n",
              "45739       0       5.0          2.0      160-169    18171.0    1.0  \n",
              "45740       0       5.0          2.0      160-169    18171.0    1.0  \n",
              "45741       0       5.0          2.0      160-169    18171.0    1.0  \n",
              "\n",
              "[25852 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a2b8056a-00fc-4569-9125-5fb89d364e8d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>admissionid</th>\n",
              "      <th>binn</th>\n",
              "      <th>fluid_sum</th>\n",
              "      <th>vasops_sum</th>\n",
              "      <th>time</th>\n",
              "      <th>index</th>\n",
              "      <th>Urine</th>\n",
              "      <th>Kreatinine</th>\n",
              "      <th>Kreatinine (bloed)</th>\n",
              "      <th>KREAT enzym. (bloed)</th>\n",
              "      <th>...</th>\n",
              "      <th>Cl (onv.ISE) (bloed)</th>\n",
              "      <th>Niet invasieve bloeddruk gemiddeld</th>\n",
              "      <th>ABP gemiddeld II</th>\n",
              "      <th>ABP gemiddeld</th>\n",
              "      <th>gender</th>\n",
              "      <th>agegroup</th>\n",
              "      <th>weightgroup</th>\n",
              "      <th>heightgroup</th>\n",
              "      <th>patientid</th>\n",
              "      <th>death</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>1564.114410</td>\n",
              "      <td>127.987797</td>\n",
              "      <td>1970-01-01 00:00:00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.666667</td>\n",
              "      <td>439.0</td>\n",
              "      <td>342.2</td>\n",
              "      <td>331.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>114.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180-189</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>240</td>\n",
              "      <td>1081.298935</td>\n",
              "      <td>189.248434</td>\n",
              "      <td>1970-01-01 04:00:00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>10.666667</td>\n",
              "      <td>439.0</td>\n",
              "      <td>337.0</td>\n",
              "      <td>331.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>114.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>72.400000</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180-189</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>480</td>\n",
              "      <td>564.661661</td>\n",
              "      <td>198.434211</td>\n",
              "      <td>1970-01-01 08:00:00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>10.666667</td>\n",
              "      <td>439.0</td>\n",
              "      <td>316.0</td>\n",
              "      <td>331.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>114.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>71.200000</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180-189</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11</td>\n",
              "      <td>720</td>\n",
              "      <td>4650.132862</td>\n",
              "      <td>155.536134</td>\n",
              "      <td>1970-01-01 12:00:00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>9.633333</td>\n",
              "      <td>439.0</td>\n",
              "      <td>302.0</td>\n",
              "      <td>331.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>114.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>66.750000</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180-189</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>960</td>\n",
              "      <td>1158.447309</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>1970-01-01 16:00:00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>439.0</td>\n",
              "      <td>302.0</td>\n",
              "      <td>331.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>114.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>63.800000</td>\n",
              "      <td>0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>180-189</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45737</th>\n",
              "      <td>21038</td>\n",
              "      <td>4080</td>\n",
              "      <td>6249.225680</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1970-01-03 20:00:00</td>\n",
              "      <td>39364.0</td>\n",
              "      <td>860.000000</td>\n",
              "      <td>281.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>118.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>160-169</td>\n",
              "      <td>18163.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45738</th>\n",
              "      <td>21049</td>\n",
              "      <td>0</td>\n",
              "      <td>8151.945003</td>\n",
              "      <td>13.393176</td>\n",
              "      <td>1970-01-01 00:00:00</td>\n",
              "      <td>39365.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>281.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>59.285714</td>\n",
              "      <td>...</td>\n",
              "      <td>118.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>68.857143</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>160-169</td>\n",
              "      <td>18171.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45739</th>\n",
              "      <td>21049</td>\n",
              "      <td>240</td>\n",
              "      <td>590.869801</td>\n",
              "      <td>18.032787</td>\n",
              "      <td>1970-01-01 04:00:00</td>\n",
              "      <td>39366.0</td>\n",
              "      <td>218.000000</td>\n",
              "      <td>281.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>118.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>67.500000</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>160-169</td>\n",
              "      <td>18171.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45740</th>\n",
              "      <td>21049</td>\n",
              "      <td>480</td>\n",
              "      <td>2821.231277</td>\n",
              "      <td>17.821038</td>\n",
              "      <td>1970-01-01 08:00:00</td>\n",
              "      <td>39367.0</td>\n",
              "      <td>830.000000</td>\n",
              "      <td>281.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>71.714286</td>\n",
              "      <td>...</td>\n",
              "      <td>118.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>69.714286</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>160-169</td>\n",
              "      <td>18171.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45741</th>\n",
              "      <td>21049</td>\n",
              "      <td>720</td>\n",
              "      <td>1255.125398</td>\n",
              "      <td>17.899379</td>\n",
              "      <td>1970-01-01 12:00:00</td>\n",
              "      <td>39368.0</td>\n",
              "      <td>128.666667</td>\n",
              "      <td>281.0</td>\n",
              "      <td>81.0</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>118.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>99.0</td>\n",
              "      <td>71.200000</td>\n",
              "      <td>0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>160-169</td>\n",
              "      <td>18171.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25852 rows Ã— 33 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2b8056a-00fc-4569-9125-5fb89d364e8d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a2b8056a-00fc-4569-9125-5fb89d364e8d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a2b8056a-00fc-4569-9125-5fb89d364e8d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Save Aggregated Dataframe on Drive"
      ],
      "metadata": {
        "id": "muHoNB95Ofum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated.to_csv('aggregated.csv')"
      ],
      "metadata": {
        "id": "8NYG5CrvOmq1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Plot Statistics"
      ],
      "metadata": {
        "id": "9sF8r0eVlXk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot some stats, df with 1 row per patient for demographic stats\n",
        "stats = state.drop_duplicates(subset=['admissionid'], keep='first')"
      ],
      "metadata": {
        "id": "GlLzI0-GlJDk"
      },
      "execution_count": 56,
      "outputs": []
    }
  ]
}