{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Preparation",
      "provenance": [],
      "authorship_tag": "ABX9TyNlLqnWaH1GGSXzWPbu3Vq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Enrico-Call/RL-AKI/blob/main/3_Data_Preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/AmsterdamUMC/AmsterdamUMCdb/blob/master/img/logo_amds.png?raw=1\" alt=\"Logo\" width=128px/>\n",
        "\n",
        "# VUmc Research Project - Reinforcement Learning for Sepsis Prevention\n",
        "# Data Preparation\n",
        "\n",
        "AmsterdamUMCdb version 1.0.2 March 2020  \n",
        "Copyright &copy; 2003-2022 Amsterdam UMC - Amsterdam Medical Data Science"
      ],
      "metadata": {
        "id": "TnLq6gcTMepC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load Data and Set Up Environment"
      ],
      "metadata": {
        "id": "M73KRmDtMmsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyclust\n",
        "!pip install treelib\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numbers\n",
        "import sys\n",
        "import sklearn\n",
        "import itertools\n",
        "import re\n",
        "import dateutil\n",
        "import random\n",
        "import scipy\n",
        "import pyclust\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.colors as cl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as md\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "from sklearn.neighbors import DistanceMetric\n",
        "from pathlib import Path\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from scipy.optimize import curve_fit\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from nltk.cluster.kmeans import KMeansClusterer\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir('/content/drive/MyDrive/MLRFH')\n",
        "\n",
        "aggregated = pd.read_csv('aggregated.csv')"
      ],
      "metadata": {
        "id": "fIB6m_wvs-Oz",
        "outputId": "e5d637ee-e72d-4441-a375-a8f810290ef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyclust in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: treelib in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from treelib) (0.16.0)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Clustering"
      ],
      "metadata": {
        "id": "J6Bc1fhOs_J6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Define Helper Functions \n",
        "\n",
        "From Mark Hoogendoorn and Burkhardt Funk (2017), Machine Learning for the Quantified Self, Springer         "
      ],
      "metadata": {
        "id": "svgeFfC3ZQEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualizeDataset:\n",
        "\n",
        "    point_displays = ['+', 'x'] #'*', 'd', 'o', 's', '<', '>']\n",
        "    line_displays = ['-'] #, '--', ':', '-.']\n",
        "    colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k']\n",
        "\n",
        "    # Set some initial attributes to define and create a save location for the images.\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # Plot the dataset, here columns can specify a specific attribute, but also a generic name that occurs\n",
        "    # among multiple attributes (e.g. label which occurs as labelWalking, etc). In such a case they are plotted\n",
        "    # in the same graph. The display should express whether points or a line should be plotted.\n",
        "    # Match can be 'exact' or 'like'. Display can be 'points' or 'line'.\n",
        "    def plot_dataset(self, data_table, columns, match='like', display='line'):\n",
        "        names = list(data_table.columns)\n",
        "\n",
        "        # Create subplots if more columns are specified.\n",
        "        if len(columns) > 1:\n",
        "            f, xar = plt.subplots(len(columns), sharex=True, sharey=False)\n",
        "        else:\n",
        "            f, xar = plt.subplots()\n",
        "            xar = [xar]\n",
        "\n",
        "        f.subplots_adjust(hspace=0.4)\n",
        "\n",
        "        xfmt = md.DateFormatter('%H:%M')\n",
        "\n",
        "        # Pass through the columns specified.\n",
        "        for i in range(0, len(columns)):\n",
        "            xar[i].xaxis.set_major_formatter(xfmt)\n",
        "            xar[i].set_prop_cycle(color=['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n",
        "            # if a column match is specified as 'exact', select the column name(s) with an exact match.\n",
        "            # If it's specified as 'like', select columns containing the name.\n",
        "\n",
        "            # We can match exact (i.e. a columns name is an exact name of a columns or 'like' for\n",
        "            # which we need to find columns names in the dataset that contain the name.\n",
        "            if match[i] == 'exact':\n",
        "                relevant_cols = [columns[i]]\n",
        "            elif match[i] == 'like':\n",
        "                relevant_cols = [name for name in names if columns[i] == name[0:len(columns[i])]]\n",
        "            else:\n",
        "                raise ValueError(\"Match should be 'exact' or 'like' for \" + str(i) + \".\")\n",
        "\n",
        "            max_values = []\n",
        "            min_values = []\n",
        "\n",
        "\n",
        "\n",
        "            # Pass through the relevant columns.\n",
        "            for j in range(0, len(relevant_cols)):\n",
        "                # Create a mask to ignore the NaN and Inf values when plotting:\n",
        "                mask = data_table[relevant_cols[j]].replace([np.inf, -np.inf], np.nan).notnull()\n",
        "                max_values.append(data_table[relevant_cols[j]][mask].max())\n",
        "                min_values.append(data_table[relevant_cols[j]][mask].min())\n",
        "\n",
        "                # Display point, or as a line\n",
        "                if display[i] == 'points':\n",
        "                    xar[i].plot(data_table.index[mask], data_table[relevant_cols[j]][mask],\n",
        "                                self.point_displays[j%len(self.point_displays)])\n",
        "                else:\n",
        "                    xar[i].plot(data_table.index[mask], data_table[relevant_cols[j]][mask],\n",
        "                                self.line_displays[j%len(self.line_displays)])\n",
        "\n",
        "            xar[i].tick_params(axis='y', labelsize=10)\n",
        "            xar[i].legend(relevant_cols, fontsize='xx-small', numpoints=1, loc='upper center',\n",
        "                          bbox_to_anchor=(0.5, 1.3), ncol=len(relevant_cols), fancybox=True, shadow=True)\n",
        "\n",
        "            xar[i].set_ylim([min(min_values) - 0.1*(max(max_values) - min(min_values)),\n",
        "                             max(max_values) + 0.1*(max(max_values) - min(min_values))])\n",
        "\n",
        "        # Make sure we get a nice figure with only a single x-axis and labels there.\n",
        "        plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
        "        plt.xlabel('time')\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_xy(self, x, y, method='plot', xlabel=None, ylabel=None, xlim=None, ylim=None, names=None,\n",
        "                line_styles=None, loc=None, title=None):\n",
        "        for input in x, y:\n",
        "            if not hasattr(input[0], '__iter__'):\n",
        "                raise TypeError('x/y should be given as a list of lists of coordinates')\n",
        "\n",
        "        plot_method = getattr(plt, method)\n",
        "        for i, (x_line, y_line) in enumerate(zip(x, y)):\n",
        "\n",
        "            plot_method(x_line, y_line, line_styles[i]) if line_styles is not None else plt.plot(x_line, y_line)\n",
        "\n",
        "            if xlabel is not None: plt.xlabel(xlabel)\n",
        "            if ylabel is not None: plt.ylabel(ylabel)\n",
        "            if xlim is not None: plt.xlim(xlim)\n",
        "            if ylim is not None: plt.ylim(ylim)\n",
        "            if title is not None: plt.title(title)\n",
        "            if names is not None: plt.legend(names)\n",
        "\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_dataset_boxplot(self, dataset, cols):\n",
        "        plt.Figure(); dataset[cols].plot.box()\n",
        "        plt.ylim([-30,30])\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # This function plots the real and imaginary amplitudes of the frequencies found in the Fourier transformation.\n",
        "    def plot_fourier_amplitudes(self, freq, ampl_real, ampl_imag):\n",
        "        plt.xlabel('Freq(Hz)')\n",
        "        plt.ylabel('amplitude')\n",
        "        # Plot the real values as a '+' and imaginary in the same way (though with a different color).\n",
        "        plt.plot(freq, ampl_real, '+', freq, ampl_imag,'+')\n",
        "        plt.legend(['real', 'imaginary'], numpoints=1)\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot outliers in case of a binary outlier score. Here, the col specifies the real data\n",
        "    # column and outlier_col the columns with a binary value (outlier or not)\n",
        "    def plot_binary_outliers(self, data_table, col, outlier_col):\n",
        "        data_table.loc[:,:] = data_table.dropna(axis=0, subset=[col, outlier_col])\n",
        "        data_table.loc[:,outlier_col] = data_table[outlier_col].astype('bool')\n",
        "        f, xar = plt.subplots()\n",
        "        xfmt = md.DateFormatter('%H:%M')\n",
        "        xar.xaxis.set_major_formatter(xfmt)\n",
        "        plt.xlabel('time')\n",
        "        plt.ylabel('value')\n",
        "        # Plot data points that are outliers in red, and non outliers in blue.\n",
        "        xar.plot(data_table.index[data_table[outlier_col]], data_table[col][data_table[outlier_col]], 'r+')\n",
        "        xar.plot(data_table.index[~data_table[outlier_col]], data_table[col][~data_table[outlier_col]], 'b+')\n",
        "        plt.legend(['outlier ' + col, 'no_outlier_' + col], numpoints=1, fontsize='xx-small', loc='upper center',  ncol=2, fancybox=True, shadow=True)\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot values that have been imputed using one of our imputation approaches. Here, values expresses the\n",
        "    # 1 to n datasets that have resulted from value imputation.\n",
        "    def plot_imputed_values(self, data_table, names, col, *values):\n",
        "\n",
        "        xfmt = md.DateFormatter('%H:%M')\n",
        "\n",
        "        # Create proper subplots.\n",
        "        if len(values) > 0:\n",
        "            f, xar = plt.subplots(len(values) + 1, sharex=True, sharey=False)\n",
        "        else:\n",
        "            f, xar = plt.subplots()\n",
        "            xar = [xar]\n",
        "\n",
        "        f.subplots_adjust(hspace=0.4)\n",
        "\n",
        "        # plot the regular dataset.\n",
        "\n",
        "        xar[0].xaxis.set_major_formatter(xfmt)\n",
        "        xar[0].plot(data_table.index[data_table[col].notnull()], data_table[col][data_table[col].notnull()], 'b+', markersize='2')\n",
        "        xar[0].legend([names[0]], fontsize='small', numpoints=1, loc='upper center',  bbox_to_anchor=(0.5, 1.3), ncol=1, fancybox=True, shadow=True)\n",
        "\n",
        "        # and plot the others that have resulted from imputation.\n",
        "        for i in range(1, len(values)+1):\n",
        "            xar[i].xaxis.set_major_formatter(xfmt)\n",
        "            xar[i].plot(data_table.index, values[i-1], 'b+', markersize='2')\n",
        "            xar[i].legend([names[i]], fontsize='small', numpoints=1, loc='upper center',  bbox_to_anchor=(0.5, 1.3), ncol=1, fancybox=True, shadow=True)\n",
        "\n",
        "        # Diplay is nicely in subplots.\n",
        "        plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)\n",
        "        plt.xlabel('time')\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # This function plots clusters that result from the application of a clustering algorithm\n",
        "    # and also shows the class label of points. Clusters are displayed via colors, classes\n",
        "    # by means of different types of points. We assume that three data columns are clustered\n",
        "    # that do not include the label. We assume the labels to be represented by 1 or more binary\n",
        "    # columns.\n",
        "    def plot_clusters_3d(self, data_table, data_cols, cluster_col, label_cols):\n",
        "\n",
        "        color_index = 0\n",
        "        point_displays = ['+', 'x', '*', 'd', 'o', 's', '<', '>']\n",
        "\n",
        "        # Determine the number of clusters:\n",
        "        clusters = data_table[cluster_col].unique()\n",
        "        labels = []\n",
        "\n",
        "        # Get the possible labels, assuming 1 or more label columns with binary values.\n",
        "        for i in range(0, len(label_cols)):\n",
        "            labels.extend([name for name in list(data_table.columns) if label_cols[i] == name[0:len(label_cols[i])]])\n",
        "\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        handles = []\n",
        "\n",
        "        # Plot clusters individually with a certain color.\n",
        "        for cluster in clusters:\n",
        "            marker_index = 0\n",
        "            # And make sure the points of a label receive the right marker type.\n",
        "            for label in labels:\n",
        "                rows = data_table.loc[(data_table[cluster_col] == cluster) & (data_table[label] > 0)]\n",
        "                # Now we come to the assumption that there are three data_cols specified:\n",
        "                if not len(data_cols) == 3:\n",
        "                    return\n",
        "                plot_color = self.colors[color_index%len(self.colors)]\n",
        "                plot_marker = point_displays[marker_index%len(point_displays)]\n",
        "                pt = ax.scatter(rows[data_cols[0]], rows[data_cols[1]], rows[data_cols[2]], c=plot_color, marker=plot_marker)\n",
        "                if color_index == 0:\n",
        "                    handles.append(pt)\n",
        "                ax.set_xlabel(data_cols[0])\n",
        "                ax.set_ylabel(data_cols[1])\n",
        "                ax.set_zlabel(data_cols[2])\n",
        "                marker_index += 1\n",
        "            color_index += 1\n",
        "\n",
        "        plt.legend(handles, labels, fontsize='xx-small', numpoints=1)\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # This function plots the silhouettes of the different clusters that have been identified. It plots the\n",
        "    # silhouette of the individual datapoints per cluster to allow studying the clusters internally as well.\n",
        "    # For this, a column expressing the silhouette for each datapoint is assumed.\n",
        "    def plot_silhouette(self, data_table, cluster_col, silhouette_col):\n",
        "        # Taken from the examples of scikit learn\n",
        "        #(http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
        "\n",
        "        clusters = data_table[cluster_col].unique()\n",
        "\n",
        "        fig, ax1 = plt.subplots(1, 1)\n",
        "        ax1.set_xlim([-0.1, 1])\n",
        "        #ax1.set_ylim([0, len(data_table.index) + (len(clusters) + 1) * 10])\n",
        "        y_lower = 10\n",
        "        for i in range(0, len(clusters)):\n",
        "            # Aggregate the silhouette scores for samples belonging to\n",
        "            # cluster i, and sort them\n",
        "            rows = data_table.mask(data_table[cluster_col] == clusters[i])\n",
        "            ith_cluster_silhouette_values = np.array(rows[silhouette_col])\n",
        "            ith_cluster_silhouette_values.sort()\n",
        "\n",
        "            size_cluster_i = len(rows.index)\n",
        "            y_upper = y_lower + size_cluster_i\n",
        "\n",
        "            color = plt.get_cmap('Spectral')(float(i) / len(clusters))\n",
        "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "            # Label the silhouette plots with their cluster numbers at the middle\n",
        "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "            # Compute the new y_lower for next plot\n",
        "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "        ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "        # The vertical line for average silhouette score of all the values\n",
        "        ax1.axvline(x=data_table[silhouette_col].mean(), color=\"red\", linestyle=\"--\")\n",
        "\n",
        "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot a dendorgram for hierarchical clustering. It assumes that the linkage as\n",
        "    # used in sk learn is passed as an argument as well.\n",
        "    def plot_dendrogram(self, dataset, linkage):\n",
        "        sys.setrecursionlimit(40000)\n",
        "        plt.title('Hierarchical Clustering Dendrogram')\n",
        "        plt.xlabel('time points')\n",
        "        plt.ylabel('distance')\n",
        "        times = dataset.index.strftime('%H:%M:%S')\n",
        "        #dendrogram(linkage,truncate_mode='lastp',p=10, show_leaf_counts=True, leaf_rotation=90.,leaf_font_size=12.,show_contracted=True, labels=times)\n",
        "        dendrogram(linkage,truncate_mode='lastp',p=16, show_leaf_counts=True, leaf_rotation=45.,leaf_font_size=8.,show_contracted=True, labels=times)\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot the confusion matrix that has been derived in the evaluation metrics. Classes expresses the labels\n",
        "    # for the matrix. We can normalize or show the raw counts. Of course this applies to classification problems.\n",
        "    def plot_confusion_matrix(self, cm, classes, normalize=False):\n",
        "        # Taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
        "\n",
        "        # Select the colormap.\n",
        "        cmap=plt.cm.Blues\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "        plt.title('confusion matrix')\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(len(classes))\n",
        "        plt.xticks(tick_marks, classes, rotation=45)\n",
        "        plt.yticks(tick_marks, classes)\n",
        "\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        thresh = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # This function plots the predictions or an algorithms (both for the training and test set) versus the real values for\n",
        "    # a regression problem. It assumes only a single value to be predicted over a number of cases. The variables identified\n",
        "    # with reg_ are the predictions.\n",
        "    def plot_numerical_prediction_versus_real(self, train_time, train_y, regr_train_y, test_time, test_y, regr_test_y, label):\n",
        "        self.legends = {}\n",
        "\n",
        "        # Plot the values, training set cases in blue, test set in red.\n",
        "        f, xar = plt.subplots(1, 1)\n",
        "\n",
        "        xfmt = md.DateFormatter('%H:%M')\n",
        "        xar.xaxis.set_major_formatter(xfmt)\n",
        "        xar.set_prop_cycle(color=['b', 'g', 'r', 'c', 'm', 'y', 'k'])\n",
        "        plt.plot(train_time, train_y, '-', linewidth=0.5)\n",
        "        plt.plot(train_time, regr_train_y, '--', linewidth=0.5)\n",
        "\n",
        "        plt.plot(test_time, test_y, '-', linewidth=0.5)\n",
        "        plt.plot(test_time, regr_test_y, '--', linewidth=0.5)\n",
        "\n",
        "        plt.legend(['real values training', 'predicted values training', 'real values test', 'predicted values test'], loc=4)\n",
        "\n",
        "\n",
        "        # And create some fancy stuff in the figure to label the training and test set a bit clearer.\n",
        "        max_y_value = max(max(train_y.tolist()), max(regr_train_y.tolist()), max(test_y.tolist()), max(regr_test_y.tolist()))\n",
        "        min_y_value = min(min(train_y.tolist()), min(regr_train_y.tolist()), min(test_y.tolist()), min(regr_test_y.tolist()))\n",
        "        range = max_y_value - min_y_value\n",
        "        y_coord_labels = max(max(train_y.tolist()), max(regr_train_y.tolist()), max(test_y.tolist()), max(regr_test_y.tolist()))+(0.01*range)\n",
        "\n",
        "\n",
        "        plt.title('Performance of model for ' + str(label))\n",
        "        plt.ylabel(label)\n",
        "        plt.xlabel('time')\n",
        "        plt.annotate('', xy=(train_time[0],y_coord_labels), xycoords='data', xytext=(train_time[-1], y_coord_labels), textcoords='data', arrowprops={'arrowstyle': '<->'})\n",
        "        plt.annotate('training set', xy=(train_time[int(float(len(train_time))/2)], y_coord_labels*1.02), color='blue', xycoords='data', ha='center')\n",
        "        plt.annotate('', xy=(test_time[0], y_coord_labels), xycoords='data', xytext=(test_time[-1], y_coord_labels), textcoords='data', arrowprops={'arrowstyle': '<->'})\n",
        "        plt.annotate('test set', xy=(test_time[int(float(len(test_time))/2)], y_coord_labels*1.02), color='red', xycoords='data', ha='center')\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot the Pareto front for multi objective optimization problems (for the dynamical systems stuff). We consider the\n",
        "    # raw output of the MO dynamical systems approach, which includes rows with the fitness and predictions for the training\n",
        "    # and test set. We select the fitness and plot them in a graph. Note that the plot only considers the first two dimensions.\n",
        "    def plot_pareto_front(self, dynsys_output):\n",
        "        fit_1_train = []\n",
        "        fit_2_train = []\n",
        "        fit_1_test = []\n",
        "        fit_2_test = []\n",
        "        for row in dynsys_output:\n",
        "            fit_1_train.append(row[1][0])\n",
        "            fit_2_train.append(row[1][1])\n",
        "\n",
        "        plt.scatter(fit_1_train, fit_2_train, color='r')\n",
        "        plt.xlabel('mse on ' + str(dynsys_output[0][0].columns[0]))\n",
        "        plt.ylabel('mse on ' + str(dynsys_output[0][0].columns[1]))\n",
        "        #plt.savefig('{0} Example ({1}).pdf'.format(ea.__class__.__name__, problem.__class__.__name__), format='pdf')\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    # Plot a prediction for a regression model in case it concerns a multi-objective dynamical systems model. Here, we plot\n",
        "    # the individual specified. Again, the complete output of the MO approach is used as argument.\n",
        "    def plot_numerical_prediction_versus_real_dynsys_mo(self, train_time, train_y, test_time, test_y, dynsys_output, individual, label):\n",
        "        regr_train_y = dynsys_output[individual][0][label]\n",
        "        regr_test_y = dynsys_output[individual][2][label]\n",
        "        train_y = train_y[label]\n",
        "        test_y = test_y[label]\n",
        "        self.plot_numerical_prediction_versus_real(train_time, train_y, regr_train_y, test_time, test_y, regr_test_y, label)\n",
        "\n",
        "    # Visualizes the performance of different algorithms over different feature sets. Assumes the scores to contain\n",
        "    # a score on the training set followed by an sd, and the same for the test set.\n",
        "    def plot_performances(self, algs, feature_subset_names, scores_over_all_algs, ylim, std_mult, y_name):\n",
        "\n",
        "        width = float(1)/(len(feature_subset_names)+1)\n",
        "        ind = np.arange(len(algs))\n",
        "        for i in range(0, len(feature_subset_names)):\n",
        "            means = []\n",
        "            std = []\n",
        "            for j in range(0, len(algs)):\n",
        "                means.append(scores_over_all_algs[i][j][2])\n",
        "                std.append(std_mult * scores_over_all_algs[i][j][3])\n",
        "            plt.errorbar(ind + i * width, means, yerr=std, fmt=self.colors[i%len(self.colors)] + 'o', markersize='3')\n",
        "        plt.ylabel(y_name)\n",
        "        plt.xticks(ind+(float(len(feature_subset_names))/2)*width, algs)\n",
        "        plt.legend(feature_subset_names, loc=4, numpoints=1)\n",
        "        if not ylim is None:\n",
        "            plt.ylim(ylim)\n",
        "        self.save(plt)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_performances_classification(self, algs, feature_subset_names, scores_over_all_algs):\n",
        "        self.plot_performances(algs, feature_subset_names, scores_over_all_algs, [0.70, 1.0], 2, 'Accuracy')\n",
        "\n",
        "    def plot_performances_regression(self, algs, feature_subset_names, scores_over_all_algs):\n",
        "        self.plot_performances(algs, feature_subset_names, scores_over_all_algs, None, 1, 'Mean Squared Error')\n",
        "\n",
        "\n",
        "# Class defining the distance metrics that are not available as standard ones....\n",
        "class InstanceDistanceMetrics:\n",
        "\n",
        "    # S for gowers distance\n",
        "    def s(self, val1, val2, range):\n",
        "        # If we compare numbers we look at the difference and normalize.\n",
        "        if isinstance(val1, numbers.Number) and isinstance(val1, numbers.Number):\n",
        "            return 1 - (float(abs(val1-val2))/range)\n",
        "        # If we compare something else, we just look at whether they are equal.\n",
        "        else:\n",
        "            if val1 == val2:\n",
        "                return 1\n",
        "            else:\n",
        "                return 0\n",
        "\n",
        "    # Delta for gowers distance.\n",
        "    def delta(self, val1, val2):\n",
        "        # Check whether both values are known (i.e. nan), if so the delta is 1, 0 otherwise.\n",
        "        if (not np.isnan(val1)) and (not np.isnan(val2)):\n",
        "            return 1\n",
        "        return 0\n",
        "\n",
        "    # Define gowers distance between two rows, given the ranges of the variables\n",
        "    # over the entire dataset (over all columns in row1 and row2)\n",
        "    def gower_similarity(self, data_row1, data_row2, ranges):\n",
        "        # We cannot computer if the lengths are not equal.\n",
        "        if len(data_row1.columns) != len(data_row2.columns):\n",
        "            return -1\n",
        "\n",
        "        delta_total = 0\n",
        "        s_total = 0\n",
        "\n",
        "        # iterate over all columns.\n",
        "        for i in range(0, len(data_row1.columns)):\n",
        "            val1 = data_row1[data_row1.columns[i]].values[0]\n",
        "            val2 = data_row2[data_row2.columns[i]].values[0]\n",
        "            # compute the delta\n",
        "            delta = self.delta(val1, val2)\n",
        "            delta_total = delta_total + delta\n",
        "            if delta > 0:\n",
        "                # and compute the s if the delta is above 0.\n",
        "                s_total = s_total + self.s(val1, val2, ranges[i])\n",
        "        return float(s_total)/delta_total\n",
        "\n",
        "# Class to flatten datasets or compute the statistical difference between cases.\n",
        "class PersonDistanceMetricsNoOrdering:\n",
        "\n",
        "    gower = 'gower'\n",
        "    minkowski = 'minkowski'\n",
        "\n",
        "    # This returns a dataset with aggregated data instances based on the mean values\n",
        "    # in the rows.\n",
        "    def create_instances_mean(self, datasets):\n",
        "        index = range(0, len(datasets))\n",
        "        cols = datasets[0].columns\n",
        "        new_dataset = pd.DataFrame(index=index, columns=cols)\n",
        "\n",
        "        for i in range(0, len(datasets)):\n",
        "            for col in cols:\n",
        "                # Compute the mean per column and assign that\n",
        "                # value for the row representing the current\n",
        "                # dataset.\n",
        "                new_dataset.iloc[i, new_dataset.columns.get_loc(col)] = datasets[i][col].mean()\n",
        "\n",
        "        return new_dataset\n",
        "\n",
        "    # Fit datasets to normal distribution and use parameters as instances\n",
        "    def create_instances_normal_distribution(self, datasets):\n",
        "        index = range(0, len(datasets))\n",
        "        cols = datasets[0].columns\n",
        "        new_cols = []\n",
        "        # Create new columns for the parameters of the distribution.\n",
        "        for col in cols:\n",
        "            new_cols.append(col + '_mu')\n",
        "            new_cols.append(col + '_sigma')\n",
        "        new_dataset = pd.DataFrame(index=index, columns=new_cols)\n",
        "\n",
        "        for i in range(0, len(datasets)):\n",
        "            for col in cols:\n",
        "                # Fit the distribution and assign the values to the\n",
        "                # row representing the dataset.\n",
        "                mu, sigma = norm.fit(datasets[i][col])\n",
        "                new_dataset.iloc[i, new_dataset.columns.get_loc(col + '_mu')] = mu\n",
        "                new_dataset.iloc[i, new_dataset.columns.get_loc(col + '_sigma')] = sigma\n",
        "\n",
        "        return new_dataset\n",
        "\n",
        "    # This defines the distance between datasets based on the statistical\n",
        "    # differences between the distribution we can only compute\n",
        "    # distances pairwise.\n",
        "    def p_distance(self, dataset1, dataset2):\n",
        "\n",
        "        cols = dataset1.columns\n",
        "        distance = 0\n",
        "        for col in cols:\n",
        "            D, p_value = stats.ks_2samp(dataset1[col], dataset2[col])\n",
        "            distance= distance + (1-p_value)\n",
        "        return distance\n",
        "\n",
        "# Class to compare two time ordered datasets.\n",
        "class PersonDistanceMetricsOrdering:\n",
        "\n",
        "    extreme_value = sys.float_info.max\n",
        "    tiny_value = 0.000001\n",
        "\n",
        "    # Directly pair up the datasets and computer the euclidean\n",
        "    # distances between the sequences of values.\n",
        "    def euclidean_distance(self, dataset1, dataset2):\n",
        "        dist = DistanceMetric.get_metric('euclidean')\n",
        "        if not len(dataset1.index) == len(dataset2.index):\n",
        "            return -1\n",
        "        distance = 0\n",
        "\n",
        "        for i in range(0, len(dataset1.index)):\n",
        "            data_row1 = dataset1.iloc[:,i:i+1].transpose()\n",
        "            data_row2 = dataset2.iloc[:,i:i+1].transpose()\n",
        "            ecl_dist = dist.pairwise(data_row1, data_row2)\n",
        "            distance = distance + ecl_dist\n",
        "\n",
        "        return distance\n",
        "\n",
        "    # Compute the distance between two datasets given a set lag.\n",
        "    def lag_correlation_given_lag(self, dataset1, dataset2, lag):\n",
        "        distance = 0\n",
        "        for i in range(0, len(dataset1.columns)):\n",
        "            # consider the lengths of the series, and compare the\n",
        "            # number of points in the smallest series.\n",
        "            length_ds1 = len(dataset1.index)\n",
        "            length_ds2 = len(dataset2.index) - lag\n",
        "            length_used = min(length_ds1, length_ds2)\n",
        "            if length_used < 1:\n",
        "                return self.extreme_value\n",
        "            # We multiply the values as expressed in the book.\n",
        "            ccc = np.multiply(dataset1.ix[0:length_used, i].values, dataset2.ix[lag:length_used+lag, i].values)\n",
        "            # We add the sum of the mutliplications to the distance. Correct for the difference in length.\n",
        "            distance = distance + (float(1)/(float(max(ccc.sum(), self.tiny_value))))/length_used\n",
        "        return distance\n",
        "\n",
        "    # Compute the lag correlation. For this we find the best lag.\n",
        "    def lag_correlation(self, dataset1, dataset2, max_lag):\n",
        "        best_dist = -1\n",
        "        best_lag = 0\n",
        "        for i in range(0, max_lag+1):\n",
        "            # Compute the distance given a lag.\n",
        "            current_dist = self.lag_correlation_given_lag(dataset1, dataset2, i)\n",
        "            if current_dist < best_dist or best_dist == -1:\n",
        "                best_dist = current_dist\n",
        "                best_lag = i\n",
        "        return best_dist\n",
        "\n",
        "    # Simple implementation of the dtw. Note that we use the euclidean distance here..\n",
        "    # The implementation follows the algorithm explained in the book very closely.\n",
        "    def dynamic_time_warping(self, dataset1, dataset2):\n",
        "        # Create a distance matrix between all time points.\n",
        "        cheapest_path = np.full((len(dataset1.index), len(dataset2.index)), self.extreme_value)\n",
        "        cheapest_path[0,0] = 0\n",
        "        DM = InstanceDistanceMetrics()\n",
        "\n",
        "\n",
        "        for i in range(1, len(dataset1.index)):\n",
        "            for j in range(1, len(dataset2.index)):\n",
        "                data_row1 = dataset1.iloc[i:i+1,:]\n",
        "                data_row2 = dataset2.iloc[j:j+1,:]\n",
        "                d = sklearn.metrics.pairwise.euclidean_distances(data_row1, data_row2)\n",
        "                cheapest_path[i,j] = d + min(cheapest_path[i-1, j], cheapest_path[i, j-1], cheapest_path[i-1, j-1])\n",
        "        return cheapest_path[len(dataset1.index)-1, len(dataset2.index)-1]\n",
        "\n",
        "# Implementation of the non hierarchical clustering approaches.\n",
        "class NonHierarchicalClustering:\n",
        "\n",
        "    # Global parameters for distance functions\n",
        "    p = 1\n",
        "    max_lag = 1\n",
        "\n",
        "    # Identifiers of the various distance and abstraction approaches.\n",
        "    euclidean = 'euclidean'\n",
        "    minkowski = 'minkowski'\n",
        "    manhattan = 'manhattan'\n",
        "    gower = 'gower'\n",
        "    abstraction_mean = 'abstraction_mean'\n",
        "    abstraction_normal = 'abstraction_normal'\n",
        "    abstraction_p = 'abstraction_p'\n",
        "    abstraction_euclidean = 'abstract_euclidean'\n",
        "    abstraction_lag = 'abstract_lag'\n",
        "    abstraction_dtw = 'abstract_dtw'\n",
        "\n",
        "    # Define the gowers distance between arrays to be used in k-means and k-medoids.\n",
        "    def gowers_similarity(self, X, Y=None, Y_norm_squared=None, squared=False):\n",
        "        X = np.matrix(X)\n",
        "        distances = np.zeros(shape=(X.shape[0], Y.shape[0]))\n",
        "        DM = InstanceDistanceMetrics()\n",
        "        # Pairs up the elements in the dataset\n",
        "        for x_row in range(0, X.shape[0]):\n",
        "            data_row1 = pd.DataFrame(X[x_row])\n",
        "            for y_row in range(0, Y.shape[0]):\n",
        "                data_row2 = pd.DataFrame(Y[y_row]).transpose()\n",
        "                # And computer the distance as defined in our distance metrics class.\n",
        "                distances[x_row, y_row] = DM.gowers_similarity(data_row1, data_row2, self.p)\n",
        "        return np.array(distances)\n",
        "\n",
        "    # Use a predefined distance function for the Minkowski distance\n",
        "    def minkowski_distance(self, X, Y=None, Y_norm_squared=None, squared=False):\n",
        "        dist = DistanceMetric.get_metric('minkowski', p=self.p)\n",
        "        return dist.pairwise(X, Y)\n",
        "\n",
        "    # Use a predefined distance function for the Manhattan distance\n",
        "    def manhattan_distance(self, X, Y=None, Y_norm_squared=None, squared=False):\n",
        "        dist = DistanceMetric.get_metric('manhattan')\n",
        "        return dist.pairwise(X, Y)\n",
        "\n",
        "    # Use a predefined distance function for the Euclidean distance\n",
        "    def euclidean_distance(self, X, Y=None, Y_norm_squared=None, squared=False):\n",
        "        dist = DistanceMetric.get_metric('euclidean')\n",
        "        return dist.pairwise(X, Y)\n",
        "\n",
        "    # If we want to compare dataset between persons one approach is to flatten\n",
        "    # each dataset to a single record/instance. This is done based on the approaches\n",
        "    # we have defined in the distance metrics file.\n",
        "    def aggregate_datasets(self, datasets, cols, abstraction_method):\n",
        "        temp_datasets = []\n",
        "        DM = PersonDistanceMetricsNoOrdering()\n",
        "\n",
        "        # Flatten all datasets and add them to the newly formed dataset.\n",
        "        for i in range(0, len(datasets)):\n",
        "            temp_dataset = datasets[i][cols]\n",
        "            temp_datasets.append(temp_dataset)\n",
        "\n",
        "        if abstraction_method == self.abstraction_normal:\n",
        "            return DM.create_instances_normal_distribution(temp_datasets)\n",
        "        else:\n",
        "            return DM.create_instances_mean(temp_datasets)\n",
        "\n",
        "    # Perform k-means over an individual dataset.\n",
        "    def k_means_over_instances(self, dataset, cols, k, distance_metric, max_iters, n_inits, p=1):\n",
        "\n",
        "        # Take the appropriate columns.\n",
        "        temp_dataset = dataset[cols]\n",
        "        # Override the standard distance functions. Store the original first\n",
        "        # sklearn_euclidian_distances = sklearn.cluster.k_means_.euclidean_distances\n",
        "        sklearn_euclidian_distances = sklearn.metrics.pairwise.euclidean_distances\n",
        "        if distance_metric == self.euclidean:\n",
        "            sklearn.metrics.pairwise.euclidean_distances = self.euclidean_distance\n",
        "        elif distance_metric == self.minkowski:\n",
        "            self.p = p\n",
        "            sklearn.metrics.pairwise.euclidean_distances = self.minkowski_distance\n",
        "        elif distance_metric == self.manhattan:\n",
        "            sklearn.metrics.pairwise.euclidean_distances = self.manhattan_distance\n",
        "        elif distance_metric == self.gower:\n",
        "            self.ranges = []\n",
        "            for col in temp_dataset.columns:\n",
        "                self.ranges.append(temp_dataset[col].max() - temp_dataset[col].min())\n",
        "            sklearn.metrics.pairwise.euclidean_distances = self.gower_similarity\n",
        "        # If we do not recognize the option we use the default distance function, which is much\n",
        "        # faster....\n",
        "        # Now apply the k-means algorithm\n",
        "        kmeans = KMeans(n_clusters=k, max_iter=max_iters, n_init=n_inits, random_state=0).fit(temp_dataset)\n",
        "        # Add the labels to the dataset\n",
        "        dataset['cluster'] = kmeans.labels_\n",
        "        # Compute the solhouette and add it as well.\n",
        "        silhouette_avg = silhouette_score(temp_dataset, kmeans.labels_)\n",
        "        silhouette_per_inst = silhouette_samples(temp_dataset, kmeans.labels_)\n",
        "        dataset['silhouette'] = silhouette_per_inst\n",
        "\n",
        "        # Reset the module distance function for further usage\n",
        "        sklearn_euclidian_distances = sklearn_euclidian_distances\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    # We have datasets covering multiple persons. We abstract the datatasets using an approach and create\n",
        "    # clusters of persons.\n",
        "    def k_means_over_datasets(self, datasets, cols, k, abstraction_method, distance_metric, max_iters, n_inits, p=1):\n",
        "        # Convert the datasets to instances\n",
        "        temp_dataset = self.aggregate_datasets(datasets, cols, abstraction_method)\n",
        "\n",
        "        # And simply apply the instance based algorithm.....\n",
        "        return self.k_means_over_instances(temp_dataset, temp_dataset.columns, k, distance_metric, max_iters, n_inits, p)\n",
        "\n",
        "    # For our own k-medoids algorithm we use our own implementation. For this we computer a complete distance matrix\n",
        "    # between points.\n",
        "    def compute_distance_matrix_instances(self, dataset, distance_metric):\n",
        "        # If the distance function is not defined in our distance metrics, we use the standard euclidean distance.\n",
        "        if not (distance_metric in [self.manhattan, self.minkowski, self.gower, self.euclidean]):\n",
        "            distances = sklearn.metrics.pairwise.euclidean_distances(X=dataset, Y=dataset)\n",
        "            return pd.DataFrame(distances, index=range(0, len(dataset.index)), columns=range(0, len(dataset.index)))\n",
        "        # Create an empty pandas dataframe for our distance matrix\n",
        "        distances = pd.DataFrame(index=range(0, len(dataset.index)), columns=range(0, len(dataset.index)))\n",
        "        DM = InstanceDistanceMetrics()\n",
        "\n",
        "        # Define the ranges of the columns if we use the gower distance.\n",
        "        ranges = []\n",
        "        if distance_metric == self.gower:\n",
        "            for col in dataset.columns:\n",
        "                self.ranges.append(dataset[col].max() - dataset[col].min())\n",
        "\n",
        "        # And compute the distances for each pair. Note that we assume the distances to be symmetric.\n",
        "        for i in range(0, len(dataset.index)):\n",
        "            for j in range(i, len(dataset.index)):\n",
        "                if distance_metric == self.manhattan:\n",
        "                    distances.iloc[i,j] = self.manhattan_distance(dataset.iloc[i:i+1,:], dataset.iloc[j:j+1,:])\n",
        "                elif distance_metric == self.minkowski:\n",
        "                    distances.iloc[i,j] = self.manhattan_distance(dataset.iloc[i:i+1,:], dataset.iloc[j:j+1,:], self.p)\n",
        "                elif distance_metric == self.gower:\n",
        "                    distances.iloc[i,j] = self.gowers_similarity(dataset.iloc[i:i+1,:], dataset.iloc[j:j+1,:])\n",
        "                elif distance_metric == self.euclidean:\n",
        "                    distances.iloc[i,j] = self.euclidean_distance(dataset.iloc[i:i+1,:], dataset.iloc[j:j+1,:])\n",
        "                distances.iloc[j,i] = distances.iloc[i,j]\n",
        "        return distances\n",
        "\n",
        "    # We need to implement k-medoids ourselves to accommodate all distance metrics\n",
        "    def k_medoids_over_instances(self, dataset, cols, k, distance_metric, max_iters, n_inits=5, p=1):\n",
        "        # If we set it to default we use the pyclust package...\n",
        "        temp_dataset = dataset[cols]\n",
        "        if distance_metric == 'default':\n",
        "            km = pyclust.KMedoids(n_clusters=k, n_trials=n_inits)\n",
        "            km.fit(temp_dataset.values)\n",
        "            cluster_assignment = km.labels_\n",
        "\n",
        "        else:\n",
        "            print(\"It workds\")\n",
        "            self.p = p\n",
        "            cluster_assignment = []\n",
        "            best_silhouette = -1\n",
        "\n",
        "            # Compute all distances\n",
        "            D = self.compute_distance_matrix_instances(temp_dataset, distance_metric)\n",
        "\n",
        "            for it in range(0, n_inits):\n",
        "                # First select k random points as centers:\n",
        "                centers = random.sample(range(0, len(dataset.index)), k)\n",
        "                prev_centers = []\n",
        "                points_to_cluster = []\n",
        "\n",
        "                n_iter = 0\n",
        "                while (n_iter < max_iters) and not (centers == prev_centers):\n",
        "                    n_iter += 1\n",
        "                    prev_centers = centers\n",
        "                    # Assign points to clusters.\n",
        "                    points_to_centroid = D[centers].idxmin(axis=1)\n",
        "\n",
        "                    new_centers = []\n",
        "                    for i in range(0, k):\n",
        "                    # And find the new center that minimized the sum of the differences.\n",
        "                      \n",
        "                        best_center = D.loc[points_to_centroid == centers[i]].sum().idxmin(axis=1)\n",
        "                        new_centers.append(best_center)\n",
        "                    centers = new_centers\n",
        "\n",
        "                # Convert centroids to cluster numbers:\n",
        "\n",
        "                points_to_centroid = D[centers].idxmin(axis=1)\n",
        "                current_cluster_assignment = []\n",
        "                for i in range(0, len(dataset.index)):\n",
        "                    current_cluster_assignment.append(centers.index(points_to_centroid.iloc[i]))\n",
        "\n",
        "                silhouette_avg = silhouette_score(temp_dataset, np.array(current_cluster_assignment))\n",
        "                if silhouette_avg > best_silhouette:\n",
        "                    cluster_assignment = current_cluster_assignment\n",
        "                    best_silhouette = silhouette_avg\n",
        "\n",
        "        # And add the clusters and silhouette scores to the dataset.\n",
        "        dataset['cluster'] = cluster_assignment\n",
        "        silhouette_avg = silhouette_score(temp_dataset, np.array(cluster_assignment))\n",
        "        silhouette_per_inst = silhouette_samples(temp_dataset, np.array(cluster_assignment))\n",
        "        dataset['silhouette'] = silhouette_per_inst\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    # For k-medoids we use all possible distance metrics between datasets as well. For this we\n",
        "    # again need to define a distance matrix between the datasets.\n",
        "    def compute_distance_matrix_datasets(self, datasets, distance_metric):\n",
        "        distances = pd.DataFrame(index=range(0, len(datasets)), columns=range(0, len(datasets)))\n",
        "        DMNoOrdering = PersonDistanceMetricsNoOrdering()\n",
        "        DMOrdering = PersonDistanceMetricsOrdering()\n",
        "\n",
        "        # And compute the distances for each pair. Note that we assume the distances to be symmetric.\n",
        "        for i in range(0, len(datasets)):\n",
        "            for j in range(i, len(datasets)):\n",
        "                if distance_metric == self.abstraction_p:\n",
        "                    distances.iloc[i,j] = DMNoOrdering.p_distance(datasets[i], datasets[j])\n",
        "                elif distance_metric == self.abstraction_euclidean:\n",
        "                    distances.iloc[i,j] = DMOrdering.euclidean_distance(datasets[i], datasets[j])\n",
        "                elif distance_metric == self.abstraction_lag:\n",
        "                    distances.iloc[i,j] = DMOrdering.lag_correlation(datasets[i], datasets[j], self.max_lag)\n",
        "                elif distance_metric == self.abstraction_dtw:\n",
        "                    distances.iloc[i,j] = DMOrdering.dynamic_time_warping(datasets[i], datasets[j])\n",
        "                distances.iloc[j,i] = distances.iloc[i,j]\n",
        "        return distances\n",
        "\n",
        "    # Note: distance metric only important in combination with certain abstraction methods as we allow for more\n",
        "    # in k-medoids.\n",
        "    def k_medoids_over_datasets(self, datasets, cols, k, abstraction_method, distance_metric, max_iters, n_inits=5, p=1, max_lag=5):\n",
        "        self.p = p\n",
        "        self.max_lag = max_lag\n",
        "\n",
        "        # If we compare datasets by flattening them, we can simply flatten the dataset and apply the instance based\n",
        "        # variant.\n",
        "        if abstraction_method in [self.abstraction_mean, self.abstraction_normal]:\n",
        "            # Convert the datasets to instances\n",
        "            temp_dataset = self.aggregate_datasets(datasets, cols, abstraction_method)\n",
        "\n",
        "            # And simply apply the instance based algorithm in case of\n",
        "            return self.k_medoids_over_instances(temp_dataset, temp_dataset.columns, k, distance_metric, max_iters, n_inits=n_inits, p=p)\n",
        "\n",
        "        # For the case over datasets we do not have a quality metric, therefore we just look at a single initialization for now (!)\n",
        "\n",
        "        # First select k random points as centers:\n",
        "        centers = random.sample(range(0, len(datasets)), k)\n",
        "        prev_centers = []\n",
        "        points_to_cluster = []\n",
        "        # Compute all distances\n",
        "        D = self.compute_distance_matrix_datasets(datasets, abstraction_method)\n",
        "\n",
        "        n_iter = 0\n",
        "        while (n_iter < max_iters) and not (centers == prev_centers):\n",
        "            n_iter += 1\n",
        "            prev_centers = centers\n",
        "            # Assign points to clusters.\n",
        "            points_to_centroid = D[centers].idxmin(axis=1)\n",
        "\n",
        "            new_centers = []\n",
        "            for i in range(0, k):\n",
        "                # And find the new center that minimized the sum of the differences.\n",
        "                best_center = D.loc[points_to_centroid == centers[i], points_to_centroid == centers[i]].sum().idxmin(axis=1)\n",
        "                new_centers.append(best_center)\n",
        "            centers = new_centers\n",
        "\n",
        "        # Convert centroids to cluster numbers:\n",
        "\n",
        "        points_to_centroid = D[centers].idxmin(axis=1)\n",
        "        cluster_assignment = []\n",
        "        for i in range(0, len(datasets)):\n",
        "            cluster_assignment.append(centers.index(points_to_centroid.iloc[i,:]))\n",
        "\n",
        "        dataset = pd.DataFrame(index=range(0, len(datasets)))\n",
        "        dataset['cluster'] = cluster_assignment\n",
        "\n",
        "        # Silhouette cannot be used here as it used a distance between instances, not datasets.\n",
        "\n",
        "        return dataset\n",
        "\n",
        "# In this class, we do not implement the Gover distance between instance, all others are included.\n",
        "# Furthermore, we only implement the agglomerative approach.\n",
        "class HierarchicalClustering:\n",
        "\n",
        "    link = None\n",
        "\n",
        "    # Perform agglomerative clustering over a single dataset.\n",
        "    def agglomerative_over_instances(self, dataset, cols, max_clusters, distance_metric, use_prev_linkage=False, link_function='single'):\n",
        "        temp_dataset = dataset[cols]\n",
        "        df = NonHierarchicalClustering()\n",
        "\n",
        "        if (not use_prev_linkage) or (self.link is None):\n",
        "            # Perform the clustering process according to the specified distance metric.\n",
        "            if distance_metric == df.manhattan:\n",
        "                self.link = linkage(temp_dataset.values, method=link_function, metric='cityblock')\n",
        "            else:\n",
        "                self.link = linkage(temp_dataset.values, method=link_function, metric='euclidean')\n",
        "\n",
        "        # And assign the clusters given the set maximum. In addition, compute the\n",
        "        cluster_assignment = fcluster(self.link, max_clusters, criterion='maxclust')\n",
        "        dataset['cluster'] = cluster_assignment\n",
        "        silhouette_avg = silhouette_score(temp_dataset, np.array(cluster_assignment))\n",
        "        silhouette_per_inst = silhouette_samples(temp_dataset, np.array(cluster_assignment))\n",
        "        dataset['silhouette'] = silhouette_per_inst\n",
        "\n",
        "        return dataset, self.link\n",
        "\n",
        "    # Perform agglomerative clustering over the datasets by flattening them into a single dataset.\n",
        "    def agglomerative_over_datasets(self, datasets, cols, max_clusters, abstraction_method, distance_metric, use_prev_linkage=False, link_function='single'):\n",
        "        # Convert the datasets to instances\n",
        "        df = NonHierarchicalClustering()\n",
        "        temp_dataset = df.aggregate_datasets(datasets, cols, abstraction_method)\n",
        "\n",
        "        # And simply apply the instance based algorithm...\n",
        "        return self.agglomerative_over_instances(temp_dataset, temp_dataset.columns, max_clusters, distance_metric, use_prev_linkage=use_prev_linkage, link_function=link_function)\n",
        "\n",
        "def k_means_over_instances(dataset, cols, k, max_iters, n_inits):\n",
        "\n",
        "        # Take the appropriate columns.\n",
        "        temp_dataset = dataset[cols]\n",
        "        # Now apply the k-means algorithm\n",
        "        kmeans = KMeans(n_clusters=k, max_iter=max_iters, n_init=n_inits, random_state=0).fit(temp_dataset)\n",
        "        # Add the labels to the dataset\n",
        "        dataset['cluster'] = kmeans.labels_\n",
        "        # Compute the silhouette and add it as well.\n",
        "        silhouette_avg = silhouette_score(temp_dataset, kmeans.labels_)\n",
        "        silhouette_per_inst = silhouette_samples(temp_dataset, kmeans.labels_)\n",
        "        dataset['silhouette'] = silhouette_per_inst\n",
        "\n",
        "        return dataset, silhouette_avg"
      ],
      "metadata": {
        "id": "BfqXurK-ZiPB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Perform Clustering Using K-Means"
      ],
      "metadata": {
        "id": "uudBwpj0brpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.1 First Check Best K Value (\\#clusters) based on Silhouette Scores"
      ],
      "metadata": {
        "id": "uxWFJQ3_eZgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DataViz = VisualizeDataset()\n",
        "\n",
        "clusteringNH = NonHierarchicalClustering()\n",
        "clusteringH = HierarchicalClustering()\n",
        "\n",
        "# Let us look at k-means first.\n",
        "k_values = range(5, 500, 5)\n",
        "silhouette_values = []\n",
        "\n",
        "# Do some initial runs to determine the right number for k\n",
        "\n",
        "print('===== kmeans clustering =====')\n",
        "for k in k_values:\n",
        "    print(f'k = {k}')\n",
        "    dataset_cluster = clusteringNH.k_means_over_instances(copy.deepcopy(\n",
        "        aggregated), ['Kreatinine', 'Kreatinine (bloed)', 'KREAT enzym. (bloed)',\n",
        "       'Urine', 'Chloor (bloed)', 'Natrium (bloed)',\n",
        "       'Kalium (bloed)', 'HCO3', 'Natrium', 'Natrium Astrup',\n",
        "       'Kalium Astrup', 'Chloor Astrup', 'Chloor', 'Kalium',\n",
        "       'Act.HCO3 (bloed)', 'Na (onv.ISE) (bloed)', 'K (onv.ISE) (bloed)',\n",
        "       'Cl (onv.ISE) (bloed)', 'Niet invasieve bloeddruk gemiddeld',\n",
        "       'ABP gemiddeld II', 'ABP gemiddeld', 'agegroup', 'gender',\n",
        "       'weightgroup', 'heightgroup'], k, 'default', 20, 10)\n",
        "    silhouette_score_n = dataset_cluster['silhouette'].mean()\n",
        "    # print(f'silhouette = {silhouette_score_n}')\n",
        "    silhouette_values.append(silhouette_score_n)\n",
        "\n",
        "DataViz.plot_xy(x=[k_values], y=[silhouette_values], xlabel='k', ylabel='silhouette score',\n",
        "                ylim=[0, 1], line_styles=['b-'])\n",
        "\n",
        "# And run the knn with the highest silhouette score\n",
        "\n",
        "k = k_values[np.argmax(silhouette_values)]\n",
        "print(f'Highest K-Means silhouette score: k = {k}')\n",
        "print('Use this value of k to run the --mode=final --k=?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUD3HtPjbkwd",
        "outputId": "2f80187b-7d6e-42d5-e6df-eda3038b16d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== kmeans clustering =====\n",
            "k = 5\n",
            "k = 10\n",
            "k = 15\n",
            "k = 20\n",
            "k = 25\n",
            "k = 30\n",
            "k = 35\n",
            "k = 40\n",
            "k = 45\n",
            "k = 50\n",
            "k = 55\n",
            "k = 60\n",
            "k = 65\n",
            "k = 70\n",
            "k = 75\n",
            "k = 80\n",
            "k = 85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2. Then Check Elbow Plot (Sum of Squared Distances)"
      ],
      "metadata": {
        "id": "j4sRn33WehPt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT_edsgdKf1Y"
      },
      "outputs": [],
      "source": [
        "Sum_of_squared_distances = []\n",
        "K = range(2,500, 5)\n",
        "for k in K:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km = km.fit(aggregated[['Kreatinine', 'Kreatinine (bloed)', 'KREAT enzym. (bloed)',\n",
        "       'Urine', 'Chloor (bloed)', 'Natrium (bloed)',\n",
        "       'Kalium (bloed)', 'HCO3', 'Natrium', 'Natrium Astrup',\n",
        "       'Kalium Astrup', 'Chloor Astrup', 'Chloor', 'Kalium',\n",
        "       'Act.HCO3 (bloed)', 'Na (onv.ISE) (bloed)', 'K (onv.ISE) (bloed)',\n",
        "       'Cl (onv.ISE) (bloed)', 'Niet invasieve bloeddruk gemiddeld',\n",
        "       'ABP gemiddeld II', 'ABP gemiddeld', 'agegroup', 'gender',\n",
        "       'weightgroup', 'heightgroup']])\n",
        "    Sum_of_squared_distances.append(km.inertia_)\n",
        "\n",
        "plt.plot(K, Sum_of_squared_distances, 'b-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Sum_of_squared_distances')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.3. Finally, perform K-Means Clustering using Best K Value"
      ],
      "metadata": {
        "id": "F3mRVwlXe5CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use k=50 based on previous runs\n",
        "\n",
        "new_d, sil = k_means_over_instances(aggregated, ['Kreatinine', 'Kreatinine (bloed)', 'KREAT enzym. (bloed)',\n",
        "       'Urine', 'Chloor (bloed)', 'Natrium (bloed)',\n",
        "       'Kalium (bloed)', 'HCO3', 'Natrium', 'Natrium Astrup',\n",
        "       'Kalium Astrup', 'Chloor Astrup', 'Chloor', 'Kalium',\n",
        "       'Act.HCO3 (bloed)', 'Na (onv.ISE) (bloed)', 'K (onv.ISE) (bloed)',\n",
        "       'Cl (onv.ISE) (bloed)', 'Niet invasieve bloeddruk gemiddeld',\n",
        "       'ABP gemiddeld II', 'ABP gemiddeld', 'agegroup', 'gender',\n",
        "       'weightgroup', 'heightgroup'], 50, 20, 10)"
      ],
      "metadata": {
        "id": "gptab8YHMyUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Bin Values"
      ],
      "metadata": {
        "id": "ovI_4J_nM7Mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Bin Action Space Values in 3 categories (0, Low, High)"
      ],
      "metadata": {
        "id": "vcLlFHUXfPYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning Values\n",
        "\n",
        "binsv = [-np.inf, 0, 40, np.inf]\n",
        "binsf = [-np.inf, 0.5, 1, np.inf]\n",
        "labels = [0, 1, 2]\n",
        "aggregated['vasop'] = pd.cut(aggregated['vasops_sum'], bins=binsv, labels=labels)\n",
        "aggregated['fluid'] = pd.cut(aggregated['fluid_sum'], bins=binsf, labels=labels)"
      ],
      "metadata": {
        "id": "1xlS28TYM14P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Create Actions based on combination of categoried (\\#actions = n<sup>2</sup> categories)"
      ],
      "metadata": {
        "id": "t9j74hm7fVur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 = no vasop, no fluid\n",
        "# 1 = no vasop, low fluid\n",
        "# 2 = no vasop, high fluid\n",
        "# 3 = low vasop, no fluid\n",
        "# 4 = low vasop, low fluid\n",
        "# 5 = low vasop, high fluid\n",
        "# 6 = high vasop, no fluid\n",
        "# 7 = high vasop, low fluid\n",
        "# 8 = high vasop, high fluid\n",
        "\n",
        "act = []\n",
        "\n",
        "for v, f in zip(aggregated['vasop'], aggregated['fluid']): \n",
        "  if v == 0 and f == 0: act.append('0')\n",
        "  elif v == 0 and f == 1: act.append('1')\n",
        "  elif v == 0 and f == 2: act.append('2')\n",
        "  elif v == 1 and f == 0: act.append('3')\n",
        "  elif v == 1 and f == 1: act.append('4')\n",
        "  elif v == 1 and f == 2: act.append('5')\n",
        "  elif v == 2 and f == 0: act.append('6')\n",
        "  elif v == 2 and f == 1: act.append('7')\n",
        "  elif v == 2 and f == 2: act.append('8')\n",
        "\n",
        "aggregated['action'] = act\n",
        "\n",
        "aggregated['reward'] = -aggregated['aki_stage']\n",
        "\n",
        "aggregated['next'] = aggregated['cluster'].shift(-1)\n",
        "\n",
        "final_space = aggregated.dropna()\n"
      ],
      "metadata": {
        "id": "H-6URquRM5r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Save DataFrame"
      ],
      "metadata": {
        "id": "s96n-kKZf7FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_space.to_csv('final_space.csv')"
      ],
      "metadata": {
        "id": "QFCsIH7ugABy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Check DataFrame Statistics"
      ],
      "metadata": {
        "id": "vwcRBp8PgLBD"
      }
    }
  ]
}