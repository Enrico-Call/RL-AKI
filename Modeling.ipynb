{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modeling",
      "provenance": [],
      "authorship_tag": "ABX9TyMafSAwmgtZcuLSNH4zbqGE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Enrico-Call/RL-AKI/blob/modeling-notebook/Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/AmsterdamUMC/AmsterdamUMCdb/blob/master/img/logo_amds.png?raw=1\" alt=\"Logo\" width=128px/>\n",
        "\n",
        "# VUmc Research Project - Reinforcement Learning for Sepsis Prevention\n",
        "# Data Extraction\n",
        "\n",
        "AmsterdamUMCdb version 1.0.2 March 2020  \n",
        "Copyright &copy; 2003-2022 Amsterdam UMC - Amsterdam Medical Data Science"
      ],
      "metadata": {
        "id": "Cb_YT0vk_rLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Set Up Environment and Load Data "
      ],
      "metadata": {
        "id": "WP9iu5etAvuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir('/content/drive/MyDrive/MLRFH')\n",
        "\n",
        "state_space = pd.read_csv('.csv')\n",
        "action_space = pd.read_csv('.csv')"
      ],
      "metadata": {
        "id": "wxUQAhuAAXdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Modeling\n",
        "\n",
        "Create a Q-Learning Agent to iterate over the dataset."
      ],
      "metadata": {
        "id": "eME6SrFs_t3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Transition Matrix\n",
        "\n",
        "# # Counting number of unique clusters (now we know it, but for later if we add more might be useful)\n",
        "\n",
        "# numbers = sorted(final['cluster'].unique())\n",
        "\n",
        "# # Find how many times a state is followed by another\n",
        "\n",
        "# groups = final.groupby(['cluster', 'next'])\n",
        "# counts = {i[0]:(len(i[1]) if i[0][0] != i[0][1] else 0) for i in groups} \n",
        "\n",
        "# # Build a matrix based on the counts just performed\n",
        "\n",
        "# matrix = pd.DataFrame()\n",
        "\n",
        "# for x in numbers:\n",
        "#     matrix[x] = pd.Series([counts.get((x,y), 0) for y in numbers], index=numbers)\n",
        "  \n",
        "# matrix_normed = matrix / matrix.sum(axis=0)\n",
        "# matrix = matrix_normed.to_numpy()\n",
        "# print(matrix)"
      ],
      "metadata": {
        "id": "rG68gt6yBI70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many times a state is associated with a certain reward\n",
        "\n",
        "# For now, it might make sense to model the reward of each state based on the highest number of occurrences of a certain reward\n",
        "\n",
        "# rewards = final.groupby(['cluster', 'reward']).size()\n",
        "# print(rewards)\n",
        "\n",
        "rewards = final.groupby(['cluster'], sort=False)['reward'].max()\n",
        "rewards.sort_index(inplace=True)\n",
        "rewards = rewards.to_numpy()"
      ],
      "metadata": {
        "id": "g0BMU2M1HU1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rew = final.groupby(['cluster', 'action', 'next']).size()\n",
        "# # rew.sort_index(inplace=True)\n",
        "# print(rew)\n",
        "# # rew = rew.to_numpy()"
      ],
      "metadata": {
        "id": "L4beaKx5HX3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alphas = range(0, 85, 5)  # Learning Rate\n",
        "# gamma = 0.99 # Discount Factor\n",
        "\n",
        "# for alpha in alphas:\n",
        "\n",
        "#   alpha = alpha/100\n",
        "#   val = []\n",
        "  \n",
        "#   # Q(s, a) matrix\n",
        "\n",
        "#   for i in range(10):\n",
        "\n",
        "#     Q = np.zeros((50, 4))\n",
        "\n",
        "#     from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "#     train_inds, test_inds = GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(final, groups=final['admissionid'])\n",
        "\n",
        "#     train = final.iloc[train_inds[0]]\n",
        "#     test = final.iloc[test_inds[0]]\n",
        "\n",
        "#     iterations = 1000000\n",
        "\n",
        "#     train = train[['cluster', 'action', 'next', 'reward']].sample(n = iterations, replace = True)\n",
        "#     test = test[['cluster', 'action', 'next', 'reward']].sample(n = iterations, replace = True)\n",
        "#     i = 0\n",
        "\n",
        "#     for row in train.loc[:,['cluster', 'action', 'next', 'reward']].itertuples():\n",
        "#       index, curr, act, next, rew = row\n",
        "#       delta = rew+gamma*np.max(Q[int(next), :])- Q[int(curr), int(act)]\n",
        "#       Q[int(curr), int(act)] += alpha*delta\n",
        "#       i += 1\n",
        "#       # if the update is smaller than a certain threshold, stop the iteration\n",
        "\n",
        "#       # if delta < 1e-10: break\n",
        "\n",
        "#     p_optim = 0.9\n",
        "\n",
        "#     actions = np.argmax(Q, axis = 1)\n",
        "#     pi = np.full((50, 4), ((1-p_optim)/(50-1)))\n",
        "#     for i, j in enumerate(actions):\n",
        "#       pi[i,j] = p_optim\n",
        "\n",
        "#     Q_1 = np.zeros((50, 4))\n",
        "\n",
        "#     for row in test.loc[:,['cluster', 'action', 'next', 'reward']].itertuples():\n",
        "#       index, curr, act, next, rew = row\n",
        "#       Q_1[int(curr), int(act)] += 1\n",
        "\n",
        "#     Q_1 = Q_1/Q_1.sum(axis=1, keepdims=True)\n",
        "\n",
        "#     best_act_pi = np.argmax(Q, axis=1)\n",
        "#     best_act_test = np.argmax(Q_1, axis=1)\n",
        "\n",
        "#     val.append(np.count_nonzero(best_act_pi == best_act_test))\n",
        "\n",
        "#   val = np.array(val)\n",
        "#   print(alpha, val.mean())"
      ],
      "metadata": {
        "id": "gnOrYn1OHa9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter Tuning"
      ],
      "metadata": {
        "id": "8a_cO30EIiRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphas = range(0, 85, 5)  # Learning Rate\n",
        "gamma = 0.99 # Discount Factor\n",
        "\n",
        "for alpha in alphas:\n",
        "\n",
        "  alpha = alpha/100\n",
        "\n",
        "  Q = np.zeros((50, 4))\n",
        "\n",
        "  from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "  train_inds, test_inds = GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(final, groups=final['patientid'])\n",
        "\n",
        "  train = final.iloc[train_inds[0]]\n",
        "  test = final.iloc[test_inds[0]]\n",
        "\n",
        "  iterations = 1000000\n",
        "\n",
        "  train = train[['cluster', 'action', 'next', 'reward']].sample(n = iterations, replace = True)\n",
        "  test = test[['cluster', 'action', 'next', 'reward']].sample(n = iterations, replace = True)\n",
        "  i = 0\n",
        "\n",
        "  for row in train.loc[:,['cluster', 'action', 'next', 'reward']].itertuples():\n",
        "    index, curr, act, next, rew = row\n",
        "    delta = rew+gamma*np.max(Q[int(next), :])- Q[int(curr), int(act)]\n",
        "    Q[int(curr), int(act)] += alpha*delta\n",
        "    i += 1\n",
        "    # if the update is smaller than a certain threshold, stop the iteration\n",
        "\n",
        "    # if delta < 1e-10: break\n",
        "\n",
        "  p_optim = 0.9\n",
        "\n",
        "  actions = np.argmax(Q, axis = 1)\n",
        "  pi = np.full((50, 4), ((1-p_optim)/(50-1)))\n",
        "  for i, j in enumerate(actions):\n",
        "    pi[i,j] = p_optim\n",
        "\n",
        "  Q_1 = np.zeros((50, 4))\n",
        "\n",
        "  for row in test.loc[:,['cluster', 'action', 'next', 'reward']].itertuples():\n",
        "    index, curr, act, next, rew = row\n",
        "    Q_1[int(curr), int(act)] += 1\n",
        "\n",
        "  Q_1 = Q_1/Q_1.sum(axis=1, keepdims=True)\n",
        "\n",
        "  best_act_pi = np.argmax(Q, axis=1)\n",
        "  best_act_test = np.argmax(Q_1, axis=1)\n",
        "\n",
        "  print(alpha, np.count_nonzero(best_act_pi == best_act_test))"
      ],
      "metadata": {
        "id": "emsTuJRuIeTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Model"
      ],
      "metadata": {
        "id": "a_X0dzsMJzy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "train_inds, test_inds = GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7).split(final, groups=final['admissionid'])\n",
        "\n",
        "train = final.iloc[train_inds[0]]\n",
        "test = final.iloc[test_inds[0]]\n",
        "\n",
        "train = train[['cluster', 'action', 'next', 'reward']].sample(n = iterations, replace = True)\n",
        "test = test[['cluster', 'action', 'next', 'reward']].sample(n = iterations, replace = True)\n",
        "\n",
        "alphas = 0.75  # Learning Rate\n",
        "gamma = 0.99 # Discount Factor\n",
        "\n",
        "avg_Q = []\n",
        "avg_Q1 = []\n",
        "\n",
        "for i in range(50):\n",
        "\n",
        "  Q = np.zeros((50, 4))\n",
        "\n",
        "  iterations = 1000000\n",
        "\n",
        "  i = 0\n",
        "\n",
        "  for row in train.loc[:,['cluster', 'action', 'next', 'reward']].itertuples():\n",
        "    index, curr, act, next, rew = row\n",
        "    delta = rew+gamma*np.max(Q[int(next), :])- Q[int(curr), int(act)]\n",
        "    Q[int(curr), int(act)] += alphas*delta\n",
        "    i += 1\n",
        "    # if the update is smaller than a certain threshold, stop the iteration\n",
        "\n",
        "    # if delta < 1e-10: break\n",
        "\n",
        "  p_optim = 0.9\n",
        "\n",
        "  actions = np.argmax(Q, axis = 1)\n",
        "  pi = np.full((50, 4), ((1-p_optim)/(50-1)))\n",
        "  for i, j in enumerate(actions):\n",
        "    pi[i,j] = p_optim\n",
        "\n",
        "  avg_Q.append(np.average(pi, axis=0)) "
      ],
      "metadata": {
        "id": "On6x2ZjRI9to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_Q1 = []\n",
        "\n",
        "for i in range(10):\n",
        "  Q_1 = np.zeros((50, 4))\n",
        "\n",
        "  for row in test.loc[:,['cluster', 'action', 'next', 'reward']].itertuples():\n",
        "    index, curr, act, next, rew = row\n",
        "    Q_1[int(curr), int(act)] += 1\n",
        "\n",
        "  Q_1 = Q_1/Q_1.sum(axis=1, keepdims=True)\n",
        "\n",
        "  best_act_pi = np.argmax(Q, axis=1)\n",
        "  best_act_test = np.argmax(Q_1, axis=1)\n",
        "\n",
        "  avg_Q1.append(np.average(Q_1, axis = 0))"
      ],
      "metadata": {
        "id": "kUIvbnZFJ5Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_plot = np.average(pi, axis=0)\n",
        "Q_1_plot = np.average(Q_1, axis = 0)\n",
        "\n",
        "Q_plot, Q_1_plot"
      ],
      "metadata": {
        "id": "QOp_40ojJ8yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.count_nonzero(best_act_pi == best_act_test)"
      ],
      "metadata": {
        "id": "GNETZ8HtJ_Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['0', '1', '2', '3']\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - width/2, Q_plot, width, label='Algorithm Policy')\n",
        "rects2 = ax.bar(x + width/2, Q_1_plot, width, label='Test Set Actions')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Probability of Taking Action')\n",
        "ax.set_title('Action')\n",
        "ax.set_xticks([0, 1, 2, 3])\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mni3k_qJKCYP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}